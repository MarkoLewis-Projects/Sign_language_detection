{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_model_lewis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTl41hB4Pk2V"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.models import load_model, save_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CZISNEMc006",
        "outputId": "8378dbdf-9f93-449d-8875-7ce09eb59ece"
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WFYg9d6TO0D"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/sign_mnist_train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/sign_mnist_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "lUGekrrPT4Lp",
        "outputId": "d43eeaf5-53d4-49df-dede-c5c1a76e0fad"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>pixel39</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>107</td>\n",
              "      <td>118</td>\n",
              "      <td>127</td>\n",
              "      <td>134</td>\n",
              "      <td>139</td>\n",
              "      <td>143</td>\n",
              "      <td>146</td>\n",
              "      <td>150</td>\n",
              "      <td>153</td>\n",
              "      <td>156</td>\n",
              "      <td>158</td>\n",
              "      <td>160</td>\n",
              "      <td>163</td>\n",
              "      <td>165</td>\n",
              "      <td>159</td>\n",
              "      <td>166</td>\n",
              "      <td>168</td>\n",
              "      <td>170</td>\n",
              "      <td>170</td>\n",
              "      <td>171</td>\n",
              "      <td>171</td>\n",
              "      <td>171</td>\n",
              "      <td>172</td>\n",
              "      <td>171</td>\n",
              "      <td>171</td>\n",
              "      <td>170</td>\n",
              "      <td>170</td>\n",
              "      <td>169</td>\n",
              "      <td>111</td>\n",
              "      <td>121</td>\n",
              "      <td>129</td>\n",
              "      <td>135</td>\n",
              "      <td>141</td>\n",
              "      <td>144</td>\n",
              "      <td>148</td>\n",
              "      <td>151</td>\n",
              "      <td>154</td>\n",
              "      <td>157</td>\n",
              "      <td>160</td>\n",
              "      <td>...</td>\n",
              "      <td>205</td>\n",
              "      <td>206</td>\n",
              "      <td>206</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>206</td>\n",
              "      <td>206</td>\n",
              "      <td>204</td>\n",
              "      <td>205</td>\n",
              "      <td>204</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "      <td>142</td>\n",
              "      <td>151</td>\n",
              "      <td>160</td>\n",
              "      <td>172</td>\n",
              "      <td>196</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>190</td>\n",
              "      <td>135</td>\n",
              "      <td>96</td>\n",
              "      <td>86</td>\n",
              "      <td>77</td>\n",
              "      <td>77</td>\n",
              "      <td>79</td>\n",
              "      <td>176</td>\n",
              "      <td>205</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>207</td>\n",
              "      <td>206</td>\n",
              "      <td>206</td>\n",
              "      <td>206</td>\n",
              "      <td>204</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>155</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>156</td>\n",
              "      <td>156</td>\n",
              "      <td>157</td>\n",
              "      <td>156</td>\n",
              "      <td>158</td>\n",
              "      <td>158</td>\n",
              "      <td>157</td>\n",
              "      <td>158</td>\n",
              "      <td>156</td>\n",
              "      <td>154</td>\n",
              "      <td>154</td>\n",
              "      <td>153</td>\n",
              "      <td>152</td>\n",
              "      <td>151</td>\n",
              "      <td>149</td>\n",
              "      <td>149</td>\n",
              "      <td>148</td>\n",
              "      <td>147</td>\n",
              "      <td>146</td>\n",
              "      <td>144</td>\n",
              "      <td>142</td>\n",
              "      <td>143</td>\n",
              "      <td>138</td>\n",
              "      <td>92</td>\n",
              "      <td>108</td>\n",
              "      <td>158</td>\n",
              "      <td>159</td>\n",
              "      <td>159</td>\n",
              "      <td>159</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>160</td>\n",
              "      <td>...</td>\n",
              "      <td>100</td>\n",
              "      <td>78</td>\n",
              "      <td>120</td>\n",
              "      <td>157</td>\n",
              "      <td>168</td>\n",
              "      <td>107</td>\n",
              "      <td>99</td>\n",
              "      <td>121</td>\n",
              "      <td>133</td>\n",
              "      <td>97</td>\n",
              "      <td>95</td>\n",
              "      <td>120</td>\n",
              "      <td>135</td>\n",
              "      <td>116</td>\n",
              "      <td>95</td>\n",
              "      <td>79</td>\n",
              "      <td>69</td>\n",
              "      <td>86</td>\n",
              "      <td>139</td>\n",
              "      <td>173</td>\n",
              "      <td>200</td>\n",
              "      <td>185</td>\n",
              "      <td>175</td>\n",
              "      <td>198</td>\n",
              "      <td>124</td>\n",
              "      <td>118</td>\n",
              "      <td>94</td>\n",
              "      <td>140</td>\n",
              "      <td>133</td>\n",
              "      <td>84</td>\n",
              "      <td>69</td>\n",
              "      <td>149</td>\n",
              "      <td>128</td>\n",
              "      <td>87</td>\n",
              "      <td>94</td>\n",
              "      <td>163</td>\n",
              "      <td>175</td>\n",
              "      <td>103</td>\n",
              "      <td>135</td>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>187</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>187</td>\n",
              "      <td>187</td>\n",
              "      <td>186</td>\n",
              "      <td>187</td>\n",
              "      <td>188</td>\n",
              "      <td>187</td>\n",
              "      <td>186</td>\n",
              "      <td>185</td>\n",
              "      <td>185</td>\n",
              "      <td>185</td>\n",
              "      <td>184</td>\n",
              "      <td>184</td>\n",
              "      <td>184</td>\n",
              "      <td>181</td>\n",
              "      <td>181</td>\n",
              "      <td>179</td>\n",
              "      <td>179</td>\n",
              "      <td>179</td>\n",
              "      <td>178</td>\n",
              "      <td>178</td>\n",
              "      <td>109</td>\n",
              "      <td>52</td>\n",
              "      <td>66</td>\n",
              "      <td>77</td>\n",
              "      <td>83</td>\n",
              "      <td>188</td>\n",
              "      <td>189</td>\n",
              "      <td>189</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>189</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>188</td>\n",
              "      <td>187</td>\n",
              "      <td>...</td>\n",
              "      <td>203</td>\n",
              "      <td>204</td>\n",
              "      <td>203</td>\n",
              "      <td>201</td>\n",
              "      <td>200</td>\n",
              "      <td>200</td>\n",
              "      <td>199</td>\n",
              "      <td>198</td>\n",
              "      <td>196</td>\n",
              "      <td>195</td>\n",
              "      <td>194</td>\n",
              "      <td>193</td>\n",
              "      <td>198</td>\n",
              "      <td>166</td>\n",
              "      <td>132</td>\n",
              "      <td>114</td>\n",
              "      <td>89</td>\n",
              "      <td>74</td>\n",
              "      <td>79</td>\n",
              "      <td>77</td>\n",
              "      <td>74</td>\n",
              "      <td>78</td>\n",
              "      <td>132</td>\n",
              "      <td>188</td>\n",
              "      <td>210</td>\n",
              "      <td>209</td>\n",
              "      <td>206</td>\n",
              "      <td>205</td>\n",
              "      <td>204</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "      <td>201</td>\n",
              "      <td>200</td>\n",
              "      <td>199</td>\n",
              "      <td>198</td>\n",
              "      <td>199</td>\n",
              "      <td>198</td>\n",
              "      <td>195</td>\n",
              "      <td>194</td>\n",
              "      <td>195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>211</td>\n",
              "      <td>210</td>\n",
              "      <td>211</td>\n",
              "      <td>210</td>\n",
              "      <td>210</td>\n",
              "      <td>211</td>\n",
              "      <td>209</td>\n",
              "      <td>207</td>\n",
              "      <td>208</td>\n",
              "      <td>207</td>\n",
              "      <td>206</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "      <td>201</td>\n",
              "      <td>200</td>\n",
              "      <td>198</td>\n",
              "      <td>197</td>\n",
              "      <td>195</td>\n",
              "      <td>192</td>\n",
              "      <td>197</td>\n",
              "      <td>171</td>\n",
              "      <td>51</td>\n",
              "      <td>52</td>\n",
              "      <td>54</td>\n",
              "      <td>212</td>\n",
              "      <td>213</td>\n",
              "      <td>215</td>\n",
              "      <td>215</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>213</td>\n",
              "      <td>212</td>\n",
              "      <td>212</td>\n",
              "      <td>211</td>\n",
              "      <td>211</td>\n",
              "      <td>...</td>\n",
              "      <td>247</td>\n",
              "      <td>242</td>\n",
              "      <td>233</td>\n",
              "      <td>231</td>\n",
              "      <td>230</td>\n",
              "      <td>229</td>\n",
              "      <td>227</td>\n",
              "      <td>225</td>\n",
              "      <td>223</td>\n",
              "      <td>221</td>\n",
              "      <td>220</td>\n",
              "      <td>216</td>\n",
              "      <td>58</td>\n",
              "      <td>51</td>\n",
              "      <td>49</td>\n",
              "      <td>50</td>\n",
              "      <td>57</td>\n",
              "      <td>60</td>\n",
              "      <td>17</td>\n",
              "      <td>15</td>\n",
              "      <td>18</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>159</td>\n",
              "      <td>255</td>\n",
              "      <td>237</td>\n",
              "      <td>239</td>\n",
              "      <td>237</td>\n",
              "      <td>236</td>\n",
              "      <td>235</td>\n",
              "      <td>234</td>\n",
              "      <td>233</td>\n",
              "      <td>231</td>\n",
              "      <td>230</td>\n",
              "      <td>226</td>\n",
              "      <td>225</td>\n",
              "      <td>222</td>\n",
              "      <td>229</td>\n",
              "      <td>163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13</td>\n",
              "      <td>164</td>\n",
              "      <td>167</td>\n",
              "      <td>170</td>\n",
              "      <td>172</td>\n",
              "      <td>176</td>\n",
              "      <td>179</td>\n",
              "      <td>180</td>\n",
              "      <td>184</td>\n",
              "      <td>185</td>\n",
              "      <td>186</td>\n",
              "      <td>188</td>\n",
              "      <td>189</td>\n",
              "      <td>189</td>\n",
              "      <td>190</td>\n",
              "      <td>191</td>\n",
              "      <td>189</td>\n",
              "      <td>190</td>\n",
              "      <td>190</td>\n",
              "      <td>187</td>\n",
              "      <td>190</td>\n",
              "      <td>192</td>\n",
              "      <td>193</td>\n",
              "      <td>191</td>\n",
              "      <td>191</td>\n",
              "      <td>192</td>\n",
              "      <td>192</td>\n",
              "      <td>194</td>\n",
              "      <td>194</td>\n",
              "      <td>166</td>\n",
              "      <td>169</td>\n",
              "      <td>172</td>\n",
              "      <td>174</td>\n",
              "      <td>177</td>\n",
              "      <td>180</td>\n",
              "      <td>182</td>\n",
              "      <td>185</td>\n",
              "      <td>186</td>\n",
              "      <td>187</td>\n",
              "      <td>190</td>\n",
              "      <td>...</td>\n",
              "      <td>90</td>\n",
              "      <td>77</td>\n",
              "      <td>88</td>\n",
              "      <td>117</td>\n",
              "      <td>123</td>\n",
              "      <td>127</td>\n",
              "      <td>129</td>\n",
              "      <td>134</td>\n",
              "      <td>145</td>\n",
              "      <td>152</td>\n",
              "      <td>156</td>\n",
              "      <td>179</td>\n",
              "      <td>105</td>\n",
              "      <td>106</td>\n",
              "      <td>105</td>\n",
              "      <td>104</td>\n",
              "      <td>104</td>\n",
              "      <td>104</td>\n",
              "      <td>175</td>\n",
              "      <td>199</td>\n",
              "      <td>178</td>\n",
              "      <td>152</td>\n",
              "      <td>136</td>\n",
              "      <td>130</td>\n",
              "      <td>136</td>\n",
              "      <td>150</td>\n",
              "      <td>118</td>\n",
              "      <td>92</td>\n",
              "      <td>85</td>\n",
              "      <td>76</td>\n",
              "      <td>92</td>\n",
              "      <td>105</td>\n",
              "      <td>105</td>\n",
              "      <td>108</td>\n",
              "      <td>133</td>\n",
              "      <td>163</td>\n",
              "      <td>157</td>\n",
              "      <td>163</td>\n",
              "      <td>164</td>\n",
              "      <td>179</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel1  pixel2  pixel3  ...  pixel781  pixel782  pixel783  pixel784\n",
              "0      3     107     118     127  ...       206       204       203       202\n",
              "1      6     155     157     156  ...       175       103       135       149\n",
              "2      2     187     188     188  ...       198       195       194       195\n",
              "3      2     211     211     212  ...       225       222       229       163\n",
              "4     13     164     167     170  ...       157       163       164       179\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4k7Xi4oT713",
        "outputId": "f73a9826-9291-4169-98dd-e1cee1e7df63"
      },
      "source": [
        "train['label'].nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4QSMq5xUK49"
      },
      "source": [
        "class_names = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y' ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "mdw2QON4UUaw",
        "outputId": "a4f479cb-fd11-4bf9-ef0f-e42cf00fa480"
      },
      "source": [
        "plt.imshow(train.iloc[0][1:].values.reshape(28,28))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6590a1d790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "fxEwFcZBV44O",
        "outputId": "6a4f08b5-bcbf-42fc-ec9a-3a01012ffdc0"
      },
      "source": [
        "train.iloc[:,0:1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27450</th>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27451</th>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27452</th>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27453</th>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27454</th>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27455 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       label\n",
              "0          3\n",
              "1          6\n",
              "2          2\n",
              "3          2\n",
              "4         13\n",
              "...      ...\n",
              "27450     13\n",
              "27451     23\n",
              "27452     18\n",
              "27453     17\n",
              "27454     23\n",
              "\n",
              "[27455 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzA3OTo-V06b"
      },
      "source": [
        "X_train = train.iloc[:,1:]/255\n",
        "y_train = train.iloc[:,0:1]\n",
        "y_test = test.iloc[:,0:1]\n",
        "X_test = test.iloc[:,1:]/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5Vy5yLEXpC9",
        "outputId": "53c6a1e7-f50b-4716-bcd8-9e40ecbe3c57"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27455, 784)\n",
            "(27455, 1)\n",
            "(7172, 784)\n",
            "(7172, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY45j9zvDcyM",
        "outputId": "211ab765-6c0a-4583-dcb9-75623f5c33b6"
      },
      "source": [
        "y_train.iloc[30]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label    1\n",
              "Name: 30, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "_qQyl0-gWwf1",
        "outputId": "f2b94ad8-c704-4f8e-f92c-f5ba85eca0df"
      },
      "source": [
        "plt.imshow(X_train.iloc[30].values.reshape(28,28),cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f654e199450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlNaFwSNcYGQ",
        "outputId": "2df18e74-0b26-45b3-9b40-4ae2403e1ba0"
      },
      "source": [
        "X_train = X_train.values.reshape(X_train.shape[0], *(28, 28, 1))\n",
        "X_test = X_test.values.reshape(X_test.shape[0], *(28, 28, 1))\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(27455, 28, 28, 1)\n",
            "(7172, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlydmiy_steX",
        "outputId": "987d6c64-debe-4631-e57e-9fe7837c7663"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[0.41960784],\n",
              "         [0.4627451 ],\n",
              "         [0.49803922],\n",
              "         ...,\n",
              "         [0.66666667],\n",
              "         [0.66666667],\n",
              "         [0.6627451 ]],\n",
              "\n",
              "        [[0.43529412],\n",
              "         [0.4745098 ],\n",
              "         [0.50588235],\n",
              "         ...,\n",
              "         [0.67058824],\n",
              "         [0.67058824],\n",
              "         [0.66666667]],\n",
              "\n",
              "        [[0.44313725],\n",
              "         [0.48235294],\n",
              "         [0.51372549],\n",
              "         ...,\n",
              "         [0.67058824],\n",
              "         [0.67058824],\n",
              "         [0.67058824]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.55686275],\n",
              "         [0.58823529],\n",
              "         [0.62352941],\n",
              "         ...,\n",
              "         [0.79215686],\n",
              "         [0.78823529],\n",
              "         [0.78431373]],\n",
              "\n",
              "        [[0.55686275],\n",
              "         [0.59215686],\n",
              "         [0.62745098],\n",
              "         ...,\n",
              "         [0.8       ],\n",
              "         [0.79607843],\n",
              "         [0.79215686]],\n",
              "\n",
              "        [[0.55686275],\n",
              "         [0.59215686],\n",
              "         [0.62745098],\n",
              "         ...,\n",
              "         [0.8       ],\n",
              "         [0.79607843],\n",
              "         [0.79215686]]],\n",
              "\n",
              "\n",
              "       [[[0.60784314],\n",
              "         [0.61568627],\n",
              "         [0.61176471],\n",
              "         ...,\n",
              "         [0.54117647],\n",
              "         [0.36078431],\n",
              "         [0.42352941]],\n",
              "\n",
              "        [[0.61960784],\n",
              "         [0.62352941],\n",
              "         [0.62352941],\n",
              "         ...,\n",
              "         [0.55686275],\n",
              "         [0.45490196],\n",
              "         [0.56078431]],\n",
              "\n",
              "        [[0.63137255],\n",
              "         [0.63137255],\n",
              "         [0.63137255],\n",
              "         ...,\n",
              "         [0.57647059],\n",
              "         [0.49019608],\n",
              "         [0.54901961]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.63529412],\n",
              "         [0.62352941],\n",
              "         [0.56862745],\n",
              "         ...,\n",
              "         [0.35686275],\n",
              "         [0.39607843],\n",
              "         [0.36862745]],\n",
              "\n",
              "        [[0.6       ],\n",
              "         [0.54509804],\n",
              "         [0.45098039],\n",
              "         ...,\n",
              "         [0.38039216],\n",
              "         [0.37254902],\n",
              "         [0.47058824]],\n",
              "\n",
              "        [[0.52941176],\n",
              "         [0.45490196],\n",
              "         [0.37254902],\n",
              "         ...,\n",
              "         [0.40392157],\n",
              "         [0.52941176],\n",
              "         [0.58431373]]],\n",
              "\n",
              "\n",
              "       [[[0.73333333],\n",
              "         [0.7372549 ],\n",
              "         [0.7372549 ],\n",
              "         ...,\n",
              "         [0.25882353],\n",
              "         [0.30196078],\n",
              "         [0.3254902 ]],\n",
              "\n",
              "        [[0.7372549 ],\n",
              "         [0.74117647],\n",
              "         [0.74117647],\n",
              "         ...,\n",
              "         [0.28627451],\n",
              "         [0.28627451],\n",
              "         [0.27843137]],\n",
              "\n",
              "        [[0.74509804],\n",
              "         [0.74509804],\n",
              "         [0.74509804],\n",
              "         ...,\n",
              "         [0.29019608],\n",
              "         [0.26666667],\n",
              "         [0.23921569]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.83137255],\n",
              "         [0.77647059],\n",
              "         [0.68627451],\n",
              "         ...,\n",
              "         [0.76470588],\n",
              "         [0.75686275],\n",
              "         [0.75294118]],\n",
              "\n",
              "        [[0.79215686],\n",
              "         [0.70196078],\n",
              "         [0.59607843],\n",
              "         ...,\n",
              "         [0.76470588],\n",
              "         [0.76078431],\n",
              "         [0.75686275]],\n",
              "\n",
              "        [[0.77647059],\n",
              "         [0.65098039],\n",
              "         [0.51764706],\n",
              "         ...,\n",
              "         [0.76470588],\n",
              "         [0.76078431],\n",
              "         [0.76470588]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[0.68235294],\n",
              "         [0.68235294],\n",
              "         [0.68235294],\n",
              "         ...,\n",
              "         [0.62745098],\n",
              "         [0.61960784],\n",
              "         [0.61176471]],\n",
              "\n",
              "        [[0.69803922],\n",
              "         [0.69803922],\n",
              "         [0.69411765],\n",
              "         ...,\n",
              "         [0.63529412],\n",
              "         [0.62745098],\n",
              "         [0.61960784]],\n",
              "\n",
              "        [[0.70980392],\n",
              "         [0.70588235],\n",
              "         [0.70588235],\n",
              "         ...,\n",
              "         [0.64313725],\n",
              "         [0.63529412],\n",
              "         [0.63137255]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.84313725],\n",
              "         [0.84705882],\n",
              "         [0.84705882],\n",
              "         ...,\n",
              "         [0.78823529],\n",
              "         [0.78039216],\n",
              "         [0.77647059]],\n",
              "\n",
              "        [[0.84705882],\n",
              "         [0.85098039],\n",
              "         [0.85098039],\n",
              "         ...,\n",
              "         [0.78823529],\n",
              "         [0.78431373],\n",
              "         [0.77647059]],\n",
              "\n",
              "        [[0.85098039],\n",
              "         [0.85098039],\n",
              "         [0.85098039],\n",
              "         ...,\n",
              "         [0.79215686],\n",
              "         [0.78431373],\n",
              "         [0.78431373]]],\n",
              "\n",
              "\n",
              "       [[[0.69411765],\n",
              "         [0.70980392],\n",
              "         [0.72156863],\n",
              "         ...,\n",
              "         [0.70980392],\n",
              "         [0.70196078],\n",
              "         [0.69411765]],\n",
              "\n",
              "        [[0.70196078],\n",
              "         [0.71372549],\n",
              "         [0.7254902 ],\n",
              "         ...,\n",
              "         [0.71764706],\n",
              "         [0.70980392],\n",
              "         [0.70196078]],\n",
              "\n",
              "        [[0.70980392],\n",
              "         [0.72156863],\n",
              "         [0.73333333],\n",
              "         ...,\n",
              "         [0.7254902 ],\n",
              "         [0.71764706],\n",
              "         [0.71372549]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.50588235],\n",
              "         [0.51764706],\n",
              "         [0.5254902 ],\n",
              "         ...,\n",
              "         [0.36862745],\n",
              "         [0.4       ],\n",
              "         [0.2745098 ]],\n",
              "\n",
              "        [[0.51372549],\n",
              "         [0.5254902 ],\n",
              "         [0.51764706],\n",
              "         ...,\n",
              "         [0.30588235],\n",
              "         [0.39215686],\n",
              "         [0.3372549 ]],\n",
              "\n",
              "        [[0.53333333],\n",
              "         [0.52941176],\n",
              "         [0.52941176],\n",
              "         ...,\n",
              "         [0.25098039],\n",
              "         [0.34117647],\n",
              "         [0.36470588]]],\n",
              "\n",
              "\n",
              "       [[[0.70196078],\n",
              "         [0.70588235],\n",
              "         [0.70588235],\n",
              "         ...,\n",
              "         [0.18431373],\n",
              "         [0.11764706],\n",
              "         [0.15294118]],\n",
              "\n",
              "        [[0.70588235],\n",
              "         [0.71372549],\n",
              "         [0.71764706],\n",
              "         ...,\n",
              "         [0.30980392],\n",
              "         [0.09411765],\n",
              "         [0.13333333]],\n",
              "\n",
              "        [[0.72156863],\n",
              "         [0.72156863],\n",
              "         [0.7254902 ],\n",
              "         ...,\n",
              "         [0.45098039],\n",
              "         [0.09803922],\n",
              "         [0.11372549]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.35294118],\n",
              "         [0.36862745],\n",
              "         [0.29803922],\n",
              "         ...,\n",
              "         [0.42745098],\n",
              "         [0.58823529],\n",
              "         [0.6745098 ]],\n",
              "\n",
              "        [[0.36862745],\n",
              "         [0.36470588],\n",
              "         [0.34901961],\n",
              "         ...,\n",
              "         [0.79215686],\n",
              "         [0.83529412],\n",
              "         [0.84313725]],\n",
              "\n",
              "        [[0.37647059],\n",
              "         [0.34509804],\n",
              "         [0.42745098],\n",
              "         ...,\n",
              "         [0.80392157],\n",
              "         [0.81960784],\n",
              "         [0.84313725]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5YswFN7YldD"
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWCt0I0gYqmR"
      },
      "source": [
        "model.add(Conv2D(32, (3, 3), input_shape = (28,28,1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), input_shape = (28,28,1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), input_shape = (28,28,1), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units = 512, activation = 'relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(units = 25, activation = 'softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvUh6OEBZEf6",
        "outputId": "c4b720d3-d42a-4e43-b0d0-894a3cc98e87"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 3, 3, 128)         73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 1, 1, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               66048     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 25)                12825     \n",
            "=================================================================\n",
            "Total params: 171,545\n",
            "Trainable params: 171,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZEDp3fVZgus"
      },
      "source": [
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        filepath = os.path.join('/content/drive/MyDrive/image_logs', 'lewis_image_weights.best.{epoch:03d}-{accuracy:.4f}.hdf5'),\n",
        "        monitor = 'val_accuracy', \n",
        "        save_best_only = True, \n",
        "        mode = 'max'\n",
        "        )]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOPTvL1iZH3r"
      },
      "source": [
        "model.compile(loss ='sparse_categorical_crossentropy', optimizer=SGD(learning_rate=0.01) ,metrics =['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9yzEgq6dDjo"
      },
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6Ce6oPpgbwZJ",
        "outputId": "59ede53c-ff76-45f6-c103-5f5e01f0dbec"
      },
      "source": [
        "model.fit(x=X_train,y=y_train.values,epochs=500,batch_size=512,callbacks=callbacks,validation_data=(X_test,y_test.values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            " 6/54 [==>...........................] - ETA: 1s - loss: 3.1121 - accuracy: 0.0820"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3704: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable.debug_mode()`.\n",
            "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "54/54 [==============================] - 2s 30ms/step - loss: 3.0993 - accuracy: 0.0867 - val_loss: 3.1231 - val_accuracy: 0.1018\n",
            "Epoch 2/500\n",
            "54/54 [==============================] - 1s 28ms/step - loss: 3.0881 - accuracy: 0.0911 - val_loss: 3.1126 - val_accuracy: 0.1104\n",
            "Epoch 3/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 3.0744 - accuracy: 0.0936 - val_loss: 3.0983 - val_accuracy: 0.1097\n",
            "Epoch 4/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 3.0586 - accuracy: 0.0971 - val_loss: 3.0856 - val_accuracy: 0.1143\n",
            "Epoch 5/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 3.0429 - accuracy: 0.0987 - val_loss: 3.0747 - val_accuracy: 0.1182\n",
            "Epoch 6/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 3.0220 - accuracy: 0.1010 - val_loss: 3.0563 - val_accuracy: 0.1145\n",
            "Epoch 7/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 3.0071 - accuracy: 0.1042 - val_loss: 3.0468 - val_accuracy: 0.1244\n",
            "Epoch 8/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.9884 - accuracy: 0.1060 - val_loss: 3.0234 - val_accuracy: 0.1293\n",
            "Epoch 9/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.9672 - accuracy: 0.1118 - val_loss: 3.0086 - val_accuracy: 0.1323\n",
            "Epoch 10/500\n",
            "54/54 [==============================] - 1s 28ms/step - loss: 2.9492 - accuracy: 0.1170 - val_loss: 2.9884 - val_accuracy: 0.1350\n",
            "Epoch 11/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.9337 - accuracy: 0.1160 - val_loss: 2.9746 - val_accuracy: 0.1386\n",
            "Epoch 12/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.9136 - accuracy: 0.1227 - val_loss: 2.9604 - val_accuracy: 0.1392\n",
            "Epoch 13/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.8923 - accuracy: 0.1269 - val_loss: 2.9364 - val_accuracy: 0.1454\n",
            "Epoch 14/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.8767 - accuracy: 0.1304 - val_loss: 2.9252 - val_accuracy: 0.1489\n",
            "Epoch 15/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.8550 - accuracy: 0.1330 - val_loss: 2.9014 - val_accuracy: 0.1541\n",
            "Epoch 16/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.8384 - accuracy: 0.1373 - val_loss: 2.9064 - val_accuracy: 0.1518\n",
            "Epoch 17/500\n",
            "54/54 [==============================] - 1s 28ms/step - loss: 2.8127 - accuracy: 0.1432 - val_loss: 2.8602 - val_accuracy: 0.1587\n",
            "Epoch 18/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.7965 - accuracy: 0.1483 - val_loss: 2.8385 - val_accuracy: 0.1609\n",
            "Epoch 19/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.7722 - accuracy: 0.1511 - val_loss: 2.8204 - val_accuracy: 0.1673\n",
            "Epoch 20/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.7395 - accuracy: 0.1609 - val_loss: 2.7712 - val_accuracy: 0.1741\n",
            "Epoch 21/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.7251 - accuracy: 0.1651 - val_loss: 2.7466 - val_accuracy: 0.1794\n",
            "Epoch 22/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.7018 - accuracy: 0.1694 - val_loss: 2.7392 - val_accuracy: 0.1853\n",
            "Epoch 23/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.6782 - accuracy: 0.1716 - val_loss: 2.7043 - val_accuracy: 0.1871\n",
            "Epoch 24/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.6504 - accuracy: 0.1795 - val_loss: 2.6839 - val_accuracy: 0.1992\n",
            "Epoch 25/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.6252 - accuracy: 0.1801 - val_loss: 2.6474 - val_accuracy: 0.2137\n",
            "Epoch 26/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.6027 - accuracy: 0.1889 - val_loss: 2.6146 - val_accuracy: 0.2178\n",
            "Epoch 27/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.5777 - accuracy: 0.1926 - val_loss: 2.6053 - val_accuracy: 0.2228\n",
            "Epoch 28/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.5514 - accuracy: 0.1965 - val_loss: 2.5530 - val_accuracy: 0.2444\n",
            "Epoch 29/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.5273 - accuracy: 0.2054 - val_loss: 2.5319 - val_accuracy: 0.2408\n",
            "Epoch 30/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.5065 - accuracy: 0.2062 - val_loss: 2.4986 - val_accuracy: 0.2529\n",
            "Epoch 31/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.4823 - accuracy: 0.2165 - val_loss: 2.4810 - val_accuracy: 0.2507\n",
            "Epoch 32/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 2.4587 - accuracy: 0.2198 - val_loss: 2.4480 - val_accuracy: 0.2563\n",
            "Epoch 33/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.4387 - accuracy: 0.2270 - val_loss: 2.4434 - val_accuracy: 0.2585\n",
            "Epoch 34/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.4131 - accuracy: 0.2317 - val_loss: 2.3929 - val_accuracy: 0.2651\n",
            "Epoch 35/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.3951 - accuracy: 0.2350 - val_loss: 2.3824 - val_accuracy: 0.2723\n",
            "Epoch 36/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.3729 - accuracy: 0.2406 - val_loss: 2.3463 - val_accuracy: 0.2699\n",
            "Epoch 37/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.3495 - accuracy: 0.2507 - val_loss: 2.3169 - val_accuracy: 0.2837\n",
            "Epoch 38/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.3322 - accuracy: 0.2548 - val_loss: 2.3092 - val_accuracy: 0.2886\n",
            "Epoch 39/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.3122 - accuracy: 0.2562 - val_loss: 2.2927 - val_accuracy: 0.2966\n",
            "Epoch 40/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.3009 - accuracy: 0.2642 - val_loss: 2.2660 - val_accuracy: 0.3093\n",
            "Epoch 41/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.2740 - accuracy: 0.2685 - val_loss: 2.2519 - val_accuracy: 0.3031\n",
            "Epoch 42/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.2572 - accuracy: 0.2748 - val_loss: 2.2100 - val_accuracy: 0.3207\n",
            "Epoch 43/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.2361 - accuracy: 0.2814 - val_loss: 2.2065 - val_accuracy: 0.3238\n",
            "Epoch 44/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.2197 - accuracy: 0.2836 - val_loss: 2.1796 - val_accuracy: 0.3311\n",
            "Epoch 45/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.2050 - accuracy: 0.2879 - val_loss: 2.1580 - val_accuracy: 0.3479\n",
            "Epoch 46/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.1816 - accuracy: 0.2964 - val_loss: 2.1409 - val_accuracy: 0.3567\n",
            "Epoch 47/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.1615 - accuracy: 0.3008 - val_loss: 2.1229 - val_accuracy: 0.3544\n",
            "Epoch 48/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.1491 - accuracy: 0.3092 - val_loss: 2.1090 - val_accuracy: 0.3620\n",
            "Epoch 49/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.1315 - accuracy: 0.3111 - val_loss: 2.0846 - val_accuracy: 0.3695\n",
            "Epoch 50/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.1144 - accuracy: 0.3128 - val_loss: 2.0643 - val_accuracy: 0.3664\n",
            "Epoch 51/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.0996 - accuracy: 0.3176 - val_loss: 2.0512 - val_accuracy: 0.3783\n",
            "Epoch 52/500\n",
            "54/54 [==============================] - 1s 28ms/step - loss: 2.0847 - accuracy: 0.3234 - val_loss: 2.0374 - val_accuracy: 0.3779\n",
            "Epoch 53/500\n",
            "54/54 [==============================] - 1s 28ms/step - loss: 2.0662 - accuracy: 0.3289 - val_loss: 2.0111 - val_accuracy: 0.3882\n",
            "Epoch 54/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 2.0544 - accuracy: 0.3359 - val_loss: 2.0020 - val_accuracy: 0.3917\n",
            "Epoch 55/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.0321 - accuracy: 0.3368 - val_loss: 1.9865 - val_accuracy: 0.3897\n",
            "Epoch 56/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.0200 - accuracy: 0.3442 - val_loss: 1.9695 - val_accuracy: 0.3988\n",
            "Epoch 57/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 2.0094 - accuracy: 0.3475 - val_loss: 1.9555 - val_accuracy: 0.3979\n",
            "Epoch 58/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.9943 - accuracy: 0.3536 - val_loss: 1.9336 - val_accuracy: 0.4056\n",
            "Epoch 59/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.9752 - accuracy: 0.3596 - val_loss: 1.9179 - val_accuracy: 0.4137\n",
            "Epoch 60/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.9648 - accuracy: 0.3655 - val_loss: 1.8980 - val_accuracy: 0.4225\n",
            "Epoch 61/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.9471 - accuracy: 0.3658 - val_loss: 1.8877 - val_accuracy: 0.4168\n",
            "Epoch 62/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.9293 - accuracy: 0.3704 - val_loss: 1.8668 - val_accuracy: 0.4338\n",
            "Epoch 63/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.9176 - accuracy: 0.3724 - val_loss: 1.8560 - val_accuracy: 0.4265\n",
            "Epoch 64/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.9115 - accuracy: 0.3768 - val_loss: 1.8396 - val_accuracy: 0.4307\n",
            "Epoch 65/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.8834 - accuracy: 0.3835 - val_loss: 1.8146 - val_accuracy: 0.4459\n",
            "Epoch 66/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.8799 - accuracy: 0.3869 - val_loss: 1.8001 - val_accuracy: 0.4519\n",
            "Epoch 67/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.8630 - accuracy: 0.3901 - val_loss: 1.7807 - val_accuracy: 0.4518\n",
            "Epoch 68/500\n",
            "54/54 [==============================] - 1s 28ms/step - loss: 1.8509 - accuracy: 0.3942 - val_loss: 1.7739 - val_accuracy: 0.4474\n",
            "Epoch 69/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.8370 - accuracy: 0.3992 - val_loss: 1.7550 - val_accuracy: 0.4584\n",
            "Epoch 70/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.8190 - accuracy: 0.4062 - val_loss: 1.7421 - val_accuracy: 0.4628\n",
            "Epoch 71/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.8067 - accuracy: 0.4114 - val_loss: 1.7259 - val_accuracy: 0.4820\n",
            "Epoch 72/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.7941 - accuracy: 0.4149 - val_loss: 1.7058 - val_accuracy: 0.4937\n",
            "Epoch 73/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.7823 - accuracy: 0.4182 - val_loss: 1.6991 - val_accuracy: 0.4833\n",
            "Epoch 74/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.7617 - accuracy: 0.4240 - val_loss: 1.6798 - val_accuracy: 0.4900\n",
            "Epoch 75/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.7503 - accuracy: 0.4238 - val_loss: 1.6669 - val_accuracy: 0.4941\n",
            "Epoch 76/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.7437 - accuracy: 0.4295 - val_loss: 1.6485 - val_accuracy: 0.5032\n",
            "Epoch 77/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.7318 - accuracy: 0.4325 - val_loss: 1.6380 - val_accuracy: 0.4997\n",
            "Epoch 78/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.7126 - accuracy: 0.4410 - val_loss: 1.6196 - val_accuracy: 0.5092\n",
            "Epoch 79/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.7034 - accuracy: 0.4449 - val_loss: 1.5981 - val_accuracy: 0.5198\n",
            "Epoch 80/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.6883 - accuracy: 0.4465 - val_loss: 1.5902 - val_accuracy: 0.5223\n",
            "Epoch 81/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.6734 - accuracy: 0.4503 - val_loss: 1.5727 - val_accuracy: 0.5153\n",
            "Epoch 82/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.6656 - accuracy: 0.4547 - val_loss: 1.5622 - val_accuracy: 0.5344\n",
            "Epoch 83/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.6480 - accuracy: 0.4574 - val_loss: 1.5430 - val_accuracy: 0.5402\n",
            "Epoch 84/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.6227 - accuracy: 0.4716 - val_loss: 1.5302 - val_accuracy: 0.5388\n",
            "Epoch 85/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.6216 - accuracy: 0.4661 - val_loss: 1.5189 - val_accuracy: 0.5392\n",
            "Epoch 86/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.6085 - accuracy: 0.4738 - val_loss: 1.5028 - val_accuracy: 0.5435\n",
            "Epoch 87/500\n",
            "54/54 [==============================] - 1s 28ms/step - loss: 1.5942 - accuracy: 0.4707 - val_loss: 1.4888 - val_accuracy: 0.5505\n",
            "Epoch 88/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.5781 - accuracy: 0.4814 - val_loss: 1.4736 - val_accuracy: 0.5565\n",
            "Epoch 89/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.5697 - accuracy: 0.4814 - val_loss: 1.4629 - val_accuracy: 0.5521\n",
            "Epoch 90/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.5553 - accuracy: 0.4830 - val_loss: 1.4441 - val_accuracy: 0.5646\n",
            "Epoch 91/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.5439 - accuracy: 0.4946 - val_loss: 1.4378 - val_accuracy: 0.5579\n",
            "Epoch 92/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.5262 - accuracy: 0.4971 - val_loss: 1.4202 - val_accuracy: 0.5643\n",
            "Epoch 93/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.5215 - accuracy: 0.4987 - val_loss: 1.4071 - val_accuracy: 0.5763\n",
            "Epoch 94/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.5118 - accuracy: 0.5049 - val_loss: 1.4014 - val_accuracy: 0.5605\n",
            "Epoch 95/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.4933 - accuracy: 0.5099 - val_loss: 1.3793 - val_accuracy: 0.5798\n",
            "Epoch 96/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.4789 - accuracy: 0.5130 - val_loss: 1.3723 - val_accuracy: 0.5791\n",
            "Epoch 97/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.4720 - accuracy: 0.5135 - val_loss: 1.3558 - val_accuracy: 0.5856\n",
            "Epoch 98/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.4552 - accuracy: 0.5167 - val_loss: 1.3413 - val_accuracy: 0.5831\n",
            "Epoch 99/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.4548 - accuracy: 0.5180 - val_loss: 1.3400 - val_accuracy: 0.5897\n",
            "Epoch 100/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.4291 - accuracy: 0.5321 - val_loss: 1.3221 - val_accuracy: 0.5963\n",
            "Epoch 101/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.4193 - accuracy: 0.5292 - val_loss: 1.3057 - val_accuracy: 0.5961\n",
            "Epoch 102/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.4078 - accuracy: 0.5394 - val_loss: 1.3045 - val_accuracy: 0.5973\n",
            "Epoch 103/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.3943 - accuracy: 0.5362 - val_loss: 1.2831 - val_accuracy: 0.6037\n",
            "Epoch 104/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.3892 - accuracy: 0.5406 - val_loss: 1.2799 - val_accuracy: 0.6079\n",
            "Epoch 105/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.3709 - accuracy: 0.5460 - val_loss: 1.2590 - val_accuracy: 0.6067\n",
            "Epoch 106/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.3548 - accuracy: 0.5493 - val_loss: 1.2493 - val_accuracy: 0.6189\n",
            "Epoch 107/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.3422 - accuracy: 0.5566 - val_loss: 1.2390 - val_accuracy: 0.6199\n",
            "Epoch 108/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.3358 - accuracy: 0.5532 - val_loss: 1.2275 - val_accuracy: 0.6196\n",
            "Epoch 109/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.3218 - accuracy: 0.5620 - val_loss: 1.2203 - val_accuracy: 0.6202\n",
            "Epoch 110/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.3021 - accuracy: 0.5659 - val_loss: 1.2042 - val_accuracy: 0.6313\n",
            "Epoch 111/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.3005 - accuracy: 0.5682 - val_loss: 1.1948 - val_accuracy: 0.6277\n",
            "Epoch 112/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.2862 - accuracy: 0.5722 - val_loss: 1.1872 - val_accuracy: 0.6306\n",
            "Epoch 113/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.2721 - accuracy: 0.5775 - val_loss: 1.1794 - val_accuracy: 0.6302\n",
            "Epoch 114/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.2614 - accuracy: 0.5801 - val_loss: 1.1651 - val_accuracy: 0.6390\n",
            "Epoch 115/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.2467 - accuracy: 0.5855 - val_loss: 1.1470 - val_accuracy: 0.6400\n",
            "Epoch 116/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.2370 - accuracy: 0.5869 - val_loss: 1.1350 - val_accuracy: 0.6484\n",
            "Epoch 117/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.2246 - accuracy: 0.5913 - val_loss: 1.1231 - val_accuracy: 0.6528\n",
            "Epoch 118/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.2071 - accuracy: 0.5962 - val_loss: 1.1118 - val_accuracy: 0.6570\n",
            "Epoch 119/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.2000 - accuracy: 0.5988 - val_loss: 1.1083 - val_accuracy: 0.6581\n",
            "Epoch 120/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.1852 - accuracy: 0.6037 - val_loss: 1.1045 - val_accuracy: 0.6549\n",
            "Epoch 121/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.1828 - accuracy: 0.6070 - val_loss: 1.0833 - val_accuracy: 0.6634\n",
            "Epoch 122/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.1686 - accuracy: 0.6100 - val_loss: 1.0765 - val_accuracy: 0.6645\n",
            "Epoch 123/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.1556 - accuracy: 0.6108 - val_loss: 1.0640 - val_accuracy: 0.6693\n",
            "Epoch 124/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.1592 - accuracy: 0.6106 - val_loss: 1.0608 - val_accuracy: 0.6708\n",
            "Epoch 125/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.1419 - accuracy: 0.6178 - val_loss: 1.0540 - val_accuracy: 0.6751\n",
            "Epoch 126/500\n",
            "54/54 [==============================] - 1s 28ms/step - loss: 1.1195 - accuracy: 0.6262 - val_loss: 1.0394 - val_accuracy: 0.6794\n",
            "Epoch 127/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.1133 - accuracy: 0.6246 - val_loss: 1.0220 - val_accuracy: 0.6794\n",
            "Epoch 128/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.1069 - accuracy: 0.6284 - val_loss: 1.0146 - val_accuracy: 0.6799\n",
            "Epoch 129/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.0967 - accuracy: 0.6341 - val_loss: 1.0093 - val_accuracy: 0.6870\n",
            "Epoch 130/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.0893 - accuracy: 0.6343 - val_loss: 1.0021 - val_accuracy: 0.6846\n",
            "Epoch 131/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.0717 - accuracy: 0.6388 - val_loss: 0.9810 - val_accuracy: 0.6966\n",
            "Epoch 132/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.0636 - accuracy: 0.6429 - val_loss: 0.9826 - val_accuracy: 0.6924\n",
            "Epoch 133/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.0551 - accuracy: 0.6432 - val_loss: 0.9672 - val_accuracy: 0.7032\n",
            "Epoch 134/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 1.0447 - accuracy: 0.6458 - val_loss: 0.9617 - val_accuracy: 0.6997\n",
            "Epoch 135/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.0369 - accuracy: 0.6488 - val_loss: 0.9476 - val_accuracy: 0.7040\n",
            "Epoch 136/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.0207 - accuracy: 0.6547 - val_loss: 0.9427 - val_accuracy: 0.7038\n",
            "Epoch 137/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.0104 - accuracy: 0.6587 - val_loss: 0.9268 - val_accuracy: 0.7197\n",
            "Epoch 138/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.0075 - accuracy: 0.6637 - val_loss: 0.9231 - val_accuracy: 0.7185\n",
            "Epoch 139/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 1.0026 - accuracy: 0.6581 - val_loss: 0.9082 - val_accuracy: 0.7256\n",
            "Epoch 140/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.9930 - accuracy: 0.6637 - val_loss: 0.9033 - val_accuracy: 0.7331\n",
            "Epoch 141/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.9780 - accuracy: 0.6691 - val_loss: 0.9013 - val_accuracy: 0.7189\n",
            "Epoch 142/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.9625 - accuracy: 0.6765 - val_loss: 0.8826 - val_accuracy: 0.7308\n",
            "Epoch 143/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.9599 - accuracy: 0.6760 - val_loss: 0.8880 - val_accuracy: 0.7296\n",
            "Epoch 144/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.9521 - accuracy: 0.6778 - val_loss: 0.8714 - val_accuracy: 0.7333\n",
            "Epoch 145/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.9421 - accuracy: 0.6788 - val_loss: 0.8626 - val_accuracy: 0.7363\n",
            "Epoch 146/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.9300 - accuracy: 0.6839 - val_loss: 0.8580 - val_accuracy: 0.7394\n",
            "Epoch 147/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.9291 - accuracy: 0.6864 - val_loss: 0.8510 - val_accuracy: 0.7407\n",
            "Epoch 148/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.9096 - accuracy: 0.6908 - val_loss: 0.8404 - val_accuracy: 0.7422\n",
            "Epoch 149/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.8954 - accuracy: 0.6951 - val_loss: 0.8292 - val_accuracy: 0.7493\n",
            "Epoch 150/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.8952 - accuracy: 0.6948 - val_loss: 0.8217 - val_accuracy: 0.7483\n",
            "Epoch 151/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.8817 - accuracy: 0.6961 - val_loss: 0.8183 - val_accuracy: 0.7444\n",
            "Epoch 152/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.8775 - accuracy: 0.7006 - val_loss: 0.8066 - val_accuracy: 0.7535\n",
            "Epoch 153/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.8652 - accuracy: 0.7031 - val_loss: 0.7995 - val_accuracy: 0.7535\n",
            "Epoch 154/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.8654 - accuracy: 0.7076 - val_loss: 0.7916 - val_accuracy: 0.7582\n",
            "Epoch 155/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.8525 - accuracy: 0.7145 - val_loss: 0.7892 - val_accuracy: 0.7614\n",
            "Epoch 156/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.8493 - accuracy: 0.7111 - val_loss: 0.7734 - val_accuracy: 0.7639\n",
            "Epoch 157/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.8405 - accuracy: 0.7114 - val_loss: 0.7633 - val_accuracy: 0.7606\n",
            "Epoch 158/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.8360 - accuracy: 0.7152 - val_loss: 0.7637 - val_accuracy: 0.7630\n",
            "Epoch 159/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.8314 - accuracy: 0.7156 - val_loss: 0.7617 - val_accuracy: 0.7635\n",
            "Epoch 160/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.8206 - accuracy: 0.7234 - val_loss: 0.7545 - val_accuracy: 0.7712\n",
            "Epoch 161/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.8106 - accuracy: 0.7270 - val_loss: 0.7444 - val_accuracy: 0.7690\n",
            "Epoch 162/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.7971 - accuracy: 0.7273 - val_loss: 0.7354 - val_accuracy: 0.7677\n",
            "Epoch 163/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.7958 - accuracy: 0.7308 - val_loss: 0.7195 - val_accuracy: 0.7755\n",
            "Epoch 164/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.7885 - accuracy: 0.7323 - val_loss: 0.7112 - val_accuracy: 0.7858\n",
            "Epoch 165/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.7891 - accuracy: 0.7313 - val_loss: 0.7287 - val_accuracy: 0.7733\n",
            "Epoch 166/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.7808 - accuracy: 0.7329 - val_loss: 0.7044 - val_accuracy: 0.7814\n",
            "Epoch 167/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.7800 - accuracy: 0.7326 - val_loss: 0.7020 - val_accuracy: 0.7925\n",
            "Epoch 168/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.7631 - accuracy: 0.7393 - val_loss: 0.6917 - val_accuracy: 0.7904\n",
            "Epoch 169/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.7593 - accuracy: 0.7416 - val_loss: 0.6912 - val_accuracy: 0.7864\n",
            "Epoch 170/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.7576 - accuracy: 0.7419 - val_loss: 0.6801 - val_accuracy: 0.7904\n",
            "Epoch 171/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.7401 - accuracy: 0.7452 - val_loss: 0.6747 - val_accuracy: 0.7935\n",
            "Epoch 172/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.7408 - accuracy: 0.7502 - val_loss: 0.6742 - val_accuracy: 0.7917\n",
            "Epoch 173/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.7351 - accuracy: 0.7496 - val_loss: 0.6642 - val_accuracy: 0.7953\n",
            "Epoch 174/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.7252 - accuracy: 0.7511 - val_loss: 0.6590 - val_accuracy: 0.8005\n",
            "Epoch 175/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.7187 - accuracy: 0.7518 - val_loss: 0.6511 - val_accuracy: 0.7988\n",
            "Epoch 176/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.7112 - accuracy: 0.7547 - val_loss: 0.6515 - val_accuracy: 0.7987\n",
            "Epoch 177/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.7073 - accuracy: 0.7597 - val_loss: 0.6397 - val_accuracy: 0.8040\n",
            "Epoch 178/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.7072 - accuracy: 0.7562 - val_loss: 0.6328 - val_accuracy: 0.8037\n",
            "Epoch 179/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.6974 - accuracy: 0.7627 - val_loss: 0.6328 - val_accuracy: 0.8056\n",
            "Epoch 180/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6933 - accuracy: 0.7614 - val_loss: 0.6290 - val_accuracy: 0.8016\n",
            "Epoch 181/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6829 - accuracy: 0.7644 - val_loss: 0.6168 - val_accuracy: 0.8105\n",
            "Epoch 182/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.6824 - accuracy: 0.7666 - val_loss: 0.6158 - val_accuracy: 0.8062\n",
            "Epoch 183/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6716 - accuracy: 0.7711 - val_loss: 0.6147 - val_accuracy: 0.8072\n",
            "Epoch 184/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.6681 - accuracy: 0.7697 - val_loss: 0.5990 - val_accuracy: 0.8151\n",
            "Epoch 185/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6682 - accuracy: 0.7720 - val_loss: 0.5991 - val_accuracy: 0.8134\n",
            "Epoch 186/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6623 - accuracy: 0.7728 - val_loss: 0.5931 - val_accuracy: 0.8182\n",
            "Epoch 187/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6551 - accuracy: 0.7767 - val_loss: 0.5960 - val_accuracy: 0.8118\n",
            "Epoch 188/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6474 - accuracy: 0.7781 - val_loss: 0.5877 - val_accuracy: 0.8139\n",
            "Epoch 189/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6447 - accuracy: 0.7761 - val_loss: 0.5868 - val_accuracy: 0.8160\n",
            "Epoch 190/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.6336 - accuracy: 0.7837 - val_loss: 0.5883 - val_accuracy: 0.8164\n",
            "Epoch 191/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6391 - accuracy: 0.7772 - val_loss: 0.5771 - val_accuracy: 0.8183\n",
            "Epoch 192/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6321 - accuracy: 0.7827 - val_loss: 0.5717 - val_accuracy: 0.8185\n",
            "Epoch 193/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6258 - accuracy: 0.7836 - val_loss: 0.5630 - val_accuracy: 0.8217\n",
            "Epoch 194/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.6215 - accuracy: 0.7862 - val_loss: 0.5587 - val_accuracy: 0.8232\n",
            "Epoch 195/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6218 - accuracy: 0.7900 - val_loss: 0.5593 - val_accuracy: 0.8212\n",
            "Epoch 196/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.6105 - accuracy: 0.7905 - val_loss: 0.5484 - val_accuracy: 0.8250\n",
            "Epoch 197/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.6084 - accuracy: 0.7906 - val_loss: 0.5505 - val_accuracy: 0.8249\n",
            "Epoch 198/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.6003 - accuracy: 0.7946 - val_loss: 0.5426 - val_accuracy: 0.8239\n",
            "Epoch 199/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5921 - accuracy: 0.7966 - val_loss: 0.5477 - val_accuracy: 0.8211\n",
            "Epoch 200/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.6023 - accuracy: 0.7900 - val_loss: 0.5380 - val_accuracy: 0.8264\n",
            "Epoch 201/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.5936 - accuracy: 0.7972 - val_loss: 0.5345 - val_accuracy: 0.8252\n",
            "Epoch 202/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5882 - accuracy: 0.8007 - val_loss: 0.5309 - val_accuracy: 0.8274\n",
            "Epoch 203/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5846 - accuracy: 0.7994 - val_loss: 0.5238 - val_accuracy: 0.8284\n",
            "Epoch 204/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.5816 - accuracy: 0.7977 - val_loss: 0.5265 - val_accuracy: 0.8261\n",
            "Epoch 205/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.5761 - accuracy: 0.8002 - val_loss: 0.5244 - val_accuracy: 0.8278\n",
            "Epoch 206/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.5700 - accuracy: 0.8030 - val_loss: 0.5153 - val_accuracy: 0.8305\n",
            "Epoch 207/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5731 - accuracy: 0.8017 - val_loss: 0.5116 - val_accuracy: 0.8318\n",
            "Epoch 208/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5603 - accuracy: 0.8078 - val_loss: 0.5164 - val_accuracy: 0.8291\n",
            "Epoch 209/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.5590 - accuracy: 0.8050 - val_loss: 0.5053 - val_accuracy: 0.8324\n",
            "Epoch 210/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5551 - accuracy: 0.8101 - val_loss: 0.5044 - val_accuracy: 0.8327\n",
            "Epoch 211/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5497 - accuracy: 0.8113 - val_loss: 0.5000 - val_accuracy: 0.8328\n",
            "Epoch 212/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5456 - accuracy: 0.8118 - val_loss: 0.4947 - val_accuracy: 0.8338\n",
            "Epoch 213/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5483 - accuracy: 0.8103 - val_loss: 0.4883 - val_accuracy: 0.8371\n",
            "Epoch 214/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.5408 - accuracy: 0.8152 - val_loss: 0.4868 - val_accuracy: 0.8364\n",
            "Epoch 215/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5355 - accuracy: 0.8142 - val_loss: 0.4905 - val_accuracy: 0.8344\n",
            "Epoch 216/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5384 - accuracy: 0.8154 - val_loss: 0.4834 - val_accuracy: 0.8337\n",
            "Epoch 217/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5345 - accuracy: 0.8169 - val_loss: 0.4839 - val_accuracy: 0.8353\n",
            "Epoch 218/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5266 - accuracy: 0.8210 - val_loss: 0.4744 - val_accuracy: 0.8378\n",
            "Epoch 219/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.5246 - accuracy: 0.8180 - val_loss: 0.4722 - val_accuracy: 0.8402\n",
            "Epoch 220/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5257 - accuracy: 0.8190 - val_loss: 0.4697 - val_accuracy: 0.8395\n",
            "Epoch 221/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.5171 - accuracy: 0.8217 - val_loss: 0.4635 - val_accuracy: 0.8459\n",
            "Epoch 222/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5170 - accuracy: 0.8226 - val_loss: 0.4633 - val_accuracy: 0.8390\n",
            "Epoch 223/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5120 - accuracy: 0.8240 - val_loss: 0.4658 - val_accuracy: 0.8409\n",
            "Epoch 224/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.5026 - accuracy: 0.8270 - val_loss: 0.4605 - val_accuracy: 0.8422\n",
            "Epoch 225/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.5053 - accuracy: 0.8267 - val_loss: 0.4567 - val_accuracy: 0.8424\n",
            "Epoch 226/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.4995 - accuracy: 0.8280 - val_loss: 0.4520 - val_accuracy: 0.8480\n",
            "Epoch 227/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4953 - accuracy: 0.8304 - val_loss: 0.4445 - val_accuracy: 0.8504\n",
            "Epoch 228/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4836 - accuracy: 0.8332 - val_loss: 0.4517 - val_accuracy: 0.8429\n",
            "Epoch 229/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4922 - accuracy: 0.8321 - val_loss: 0.4503 - val_accuracy: 0.8486\n",
            "Epoch 230/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4896 - accuracy: 0.8333 - val_loss: 0.4334 - val_accuracy: 0.8523\n",
            "Epoch 231/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4819 - accuracy: 0.8350 - val_loss: 0.4370 - val_accuracy: 0.8549\n",
            "Epoch 232/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4850 - accuracy: 0.8320 - val_loss: 0.4421 - val_accuracy: 0.8514\n",
            "Epoch 233/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.4816 - accuracy: 0.8348 - val_loss: 0.4368 - val_accuracy: 0.8504\n",
            "Epoch 234/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.4754 - accuracy: 0.8354 - val_loss: 0.4263 - val_accuracy: 0.8586\n",
            "Epoch 235/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.4777 - accuracy: 0.8339 - val_loss: 0.4272 - val_accuracy: 0.8607\n",
            "Epoch 236/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4685 - accuracy: 0.8382 - val_loss: 0.4270 - val_accuracy: 0.8603\n",
            "Epoch 237/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.4729 - accuracy: 0.8388 - val_loss: 0.4258 - val_accuracy: 0.8607\n",
            "Epoch 238/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.4574 - accuracy: 0.8421 - val_loss: 0.4216 - val_accuracy: 0.8642\n",
            "Epoch 239/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4541 - accuracy: 0.8470 - val_loss: 0.4178 - val_accuracy: 0.8634\n",
            "Epoch 240/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.4540 - accuracy: 0.8441 - val_loss: 0.4156 - val_accuracy: 0.8660\n",
            "Epoch 241/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4541 - accuracy: 0.8420 - val_loss: 0.4120 - val_accuracy: 0.8667\n",
            "Epoch 242/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4544 - accuracy: 0.8448 - val_loss: 0.4110 - val_accuracy: 0.8677\n",
            "Epoch 243/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.4473 - accuracy: 0.8484 - val_loss: 0.4079 - val_accuracy: 0.8687\n",
            "Epoch 244/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4470 - accuracy: 0.8469 - val_loss: 0.4074 - val_accuracy: 0.8707\n",
            "Epoch 245/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4379 - accuracy: 0.8503 - val_loss: 0.4070 - val_accuracy: 0.8684\n",
            "Epoch 246/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.4405 - accuracy: 0.8465 - val_loss: 0.4022 - val_accuracy: 0.8738\n",
            "Epoch 247/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.4356 - accuracy: 0.8502 - val_loss: 0.3999 - val_accuracy: 0.8721\n",
            "Epoch 248/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4356 - accuracy: 0.8516 - val_loss: 0.3945 - val_accuracy: 0.8753\n",
            "Epoch 249/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4292 - accuracy: 0.8525 - val_loss: 0.4017 - val_accuracy: 0.8716\n",
            "Epoch 250/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4304 - accuracy: 0.8527 - val_loss: 0.3912 - val_accuracy: 0.8802\n",
            "Epoch 251/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4260 - accuracy: 0.8541 - val_loss: 0.3913 - val_accuracy: 0.8763\n",
            "Epoch 252/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4312 - accuracy: 0.8524 - val_loss: 0.3939 - val_accuracy: 0.8756\n",
            "Epoch 253/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4280 - accuracy: 0.8556 - val_loss: 0.3946 - val_accuracy: 0.8779\n",
            "Epoch 254/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4125 - accuracy: 0.8591 - val_loss: 0.3885 - val_accuracy: 0.8797\n",
            "Epoch 255/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4127 - accuracy: 0.8577 - val_loss: 0.3781 - val_accuracy: 0.8799\n",
            "Epoch 256/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4086 - accuracy: 0.8616 - val_loss: 0.3832 - val_accuracy: 0.8791\n",
            "Epoch 257/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.4154 - accuracy: 0.8581 - val_loss: 0.3811 - val_accuracy: 0.8804\n",
            "Epoch 258/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4105 - accuracy: 0.8595 - val_loss: 0.3783 - val_accuracy: 0.8823\n",
            "Epoch 259/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4098 - accuracy: 0.8612 - val_loss: 0.3734 - val_accuracy: 0.8883\n",
            "Epoch 260/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.3974 - accuracy: 0.8663 - val_loss: 0.3758 - val_accuracy: 0.8799\n",
            "Epoch 261/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.4041 - accuracy: 0.8616 - val_loss: 0.3733 - val_accuracy: 0.8829\n",
            "Epoch 262/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3963 - accuracy: 0.8652 - val_loss: 0.3720 - val_accuracy: 0.8852\n",
            "Epoch 263/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3969 - accuracy: 0.8673 - val_loss: 0.3678 - val_accuracy: 0.8868\n",
            "Epoch 264/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3985 - accuracy: 0.8623 - val_loss: 0.3678 - val_accuracy: 0.8861\n",
            "Epoch 265/500\n",
            "54/54 [==============================] - 2s 31ms/step - loss: 0.3902 - accuracy: 0.8656 - val_loss: 0.3700 - val_accuracy: 0.8857\n",
            "Epoch 266/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3918 - accuracy: 0.8675 - val_loss: 0.3672 - val_accuracy: 0.8890\n",
            "Epoch 267/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3882 - accuracy: 0.8692 - val_loss: 0.3624 - val_accuracy: 0.8871\n",
            "Epoch 268/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3823 - accuracy: 0.8680 - val_loss: 0.3624 - val_accuracy: 0.8848\n",
            "Epoch 269/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.3801 - accuracy: 0.8703 - val_loss: 0.3598 - val_accuracy: 0.8903\n",
            "Epoch 270/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3823 - accuracy: 0.8691 - val_loss: 0.3599 - val_accuracy: 0.8850\n",
            "Epoch 271/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3795 - accuracy: 0.8702 - val_loss: 0.3540 - val_accuracy: 0.8919\n",
            "Epoch 272/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3795 - accuracy: 0.8726 - val_loss: 0.3541 - val_accuracy: 0.8921\n",
            "Epoch 273/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3716 - accuracy: 0.8757 - val_loss: 0.3533 - val_accuracy: 0.8926\n",
            "Epoch 274/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3746 - accuracy: 0.8742 - val_loss: 0.3524 - val_accuracy: 0.8914\n",
            "Epoch 275/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3771 - accuracy: 0.8712 - val_loss: 0.3499 - val_accuracy: 0.8926\n",
            "Epoch 276/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.3705 - accuracy: 0.8743 - val_loss: 0.3481 - val_accuracy: 0.8932\n",
            "Epoch 277/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3632 - accuracy: 0.8775 - val_loss: 0.3465 - val_accuracy: 0.8933\n",
            "Epoch 278/500\n",
            "54/54 [==============================] - 2s 31ms/step - loss: 0.3646 - accuracy: 0.8773 - val_loss: 0.3451 - val_accuracy: 0.8949\n",
            "Epoch 279/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3676 - accuracy: 0.8747 - val_loss: 0.3408 - val_accuracy: 0.8963\n",
            "Epoch 280/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.3612 - accuracy: 0.8777 - val_loss: 0.3443 - val_accuracy: 0.8947\n",
            "Epoch 281/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.3566 - accuracy: 0.8797 - val_loss: 0.3387 - val_accuracy: 0.8963\n",
            "Epoch 282/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.3504 - accuracy: 0.8819 - val_loss: 0.3379 - val_accuracy: 0.8957\n",
            "Epoch 283/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3541 - accuracy: 0.8793 - val_loss: 0.3364 - val_accuracy: 0.8985\n",
            "Epoch 284/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3579 - accuracy: 0.8785 - val_loss: 0.3385 - val_accuracy: 0.8970\n",
            "Epoch 285/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3516 - accuracy: 0.8805 - val_loss: 0.3385 - val_accuracy: 0.8935\n",
            "Epoch 286/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.3453 - accuracy: 0.8814 - val_loss: 0.3354 - val_accuracy: 0.8950\n",
            "Epoch 287/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3533 - accuracy: 0.8813 - val_loss: 0.3347 - val_accuracy: 0.8991\n",
            "Epoch 288/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.3520 - accuracy: 0.8805 - val_loss: 0.3311 - val_accuracy: 0.8954\n",
            "Epoch 289/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3467 - accuracy: 0.8824 - val_loss: 0.3332 - val_accuracy: 0.8977\n",
            "Epoch 290/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3380 - accuracy: 0.8863 - val_loss: 0.3212 - val_accuracy: 0.9023\n",
            "Epoch 291/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.3357 - accuracy: 0.8859 - val_loss: 0.3232 - val_accuracy: 0.9011\n",
            "Epoch 292/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3434 - accuracy: 0.8814 - val_loss: 0.3265 - val_accuracy: 0.9010\n",
            "Epoch 293/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3382 - accuracy: 0.8856 - val_loss: 0.3273 - val_accuracy: 0.8997\n",
            "Epoch 294/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3345 - accuracy: 0.8873 - val_loss: 0.3231 - val_accuracy: 0.9018\n",
            "Epoch 295/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3309 - accuracy: 0.8874 - val_loss: 0.3192 - val_accuracy: 0.9032\n",
            "Epoch 296/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3293 - accuracy: 0.8881 - val_loss: 0.3191 - val_accuracy: 0.9031\n",
            "Epoch 297/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3268 - accuracy: 0.8892 - val_loss: 0.3192 - val_accuracy: 0.9020\n",
            "Epoch 298/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3268 - accuracy: 0.8881 - val_loss: 0.3175 - val_accuracy: 0.8999\n",
            "Epoch 299/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3279 - accuracy: 0.8889 - val_loss: 0.3209 - val_accuracy: 0.9041\n",
            "Epoch 300/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3213 - accuracy: 0.8938 - val_loss: 0.3170 - val_accuracy: 0.9027\n",
            "Epoch 301/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3193 - accuracy: 0.8936 - val_loss: 0.3137 - val_accuracy: 0.9050\n",
            "Epoch 302/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3226 - accuracy: 0.8899 - val_loss: 0.3159 - val_accuracy: 0.9038\n",
            "Epoch 303/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3192 - accuracy: 0.8914 - val_loss: 0.3111 - val_accuracy: 0.9067\n",
            "Epoch 304/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3271 - accuracy: 0.8888 - val_loss: 0.3050 - val_accuracy: 0.9069\n",
            "Epoch 305/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3145 - accuracy: 0.8936 - val_loss: 0.3075 - val_accuracy: 0.9067\n",
            "Epoch 306/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3139 - accuracy: 0.8924 - val_loss: 0.3052 - val_accuracy: 0.9076\n",
            "Epoch 307/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.3151 - accuracy: 0.8917 - val_loss: 0.3058 - val_accuracy: 0.9095\n",
            "Epoch 308/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3132 - accuracy: 0.8922 - val_loss: 0.3052 - val_accuracy: 0.9077\n",
            "Epoch 309/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3104 - accuracy: 0.8940 - val_loss: 0.3077 - val_accuracy: 0.9074\n",
            "Epoch 310/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2999 - accuracy: 0.8981 - val_loss: 0.2994 - val_accuracy: 0.9085\n",
            "Epoch 311/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3083 - accuracy: 0.8950 - val_loss: 0.3020 - val_accuracy: 0.9067\n",
            "Epoch 312/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.3008 - accuracy: 0.8979 - val_loss: 0.3054 - val_accuracy: 0.9063\n",
            "Epoch 313/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3021 - accuracy: 0.8989 - val_loss: 0.3023 - val_accuracy: 0.9085\n",
            "Epoch 314/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2998 - accuracy: 0.8964 - val_loss: 0.2987 - val_accuracy: 0.9088\n",
            "Epoch 315/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3009 - accuracy: 0.9003 - val_loss: 0.2981 - val_accuracy: 0.9098\n",
            "Epoch 316/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.3024 - accuracy: 0.8967 - val_loss: 0.2979 - val_accuracy: 0.9113\n",
            "Epoch 317/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.3017 - accuracy: 0.8987 - val_loss: 0.2958 - val_accuracy: 0.9130\n",
            "Epoch 318/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2933 - accuracy: 0.9021 - val_loss: 0.2921 - val_accuracy: 0.9120\n",
            "Epoch 319/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2923 - accuracy: 0.9017 - val_loss: 0.2928 - val_accuracy: 0.9106\n",
            "Epoch 320/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2877 - accuracy: 0.9008 - val_loss: 0.2958 - val_accuracy: 0.9113\n",
            "Epoch 321/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2921 - accuracy: 0.9024 - val_loss: 0.2920 - val_accuracy: 0.9129\n",
            "Epoch 322/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2903 - accuracy: 0.9007 - val_loss: 0.2906 - val_accuracy: 0.9131\n",
            "Epoch 323/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2859 - accuracy: 0.9034 - val_loss: 0.2898 - val_accuracy: 0.9130\n",
            "Epoch 324/500\n",
            "54/54 [==============================] - 2s 31ms/step - loss: 0.2913 - accuracy: 0.9021 - val_loss: 0.2888 - val_accuracy: 0.9129\n",
            "Epoch 325/500\n",
            "54/54 [==============================] - 2s 33ms/step - loss: 0.2836 - accuracy: 0.9056 - val_loss: 0.2885 - val_accuracy: 0.9142\n",
            "Epoch 326/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2851 - accuracy: 0.9039 - val_loss: 0.2874 - val_accuracy: 0.9136\n",
            "Epoch 327/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2891 - accuracy: 0.9028 - val_loss: 0.2885 - val_accuracy: 0.9151\n",
            "Epoch 328/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2812 - accuracy: 0.9043 - val_loss: 0.2857 - val_accuracy: 0.9156\n",
            "Epoch 329/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2821 - accuracy: 0.9050 - val_loss: 0.2871 - val_accuracy: 0.9138\n",
            "Epoch 330/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2792 - accuracy: 0.9067 - val_loss: 0.2856 - val_accuracy: 0.9154\n",
            "Epoch 331/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2785 - accuracy: 0.9061 - val_loss: 0.2818 - val_accuracy: 0.9159\n",
            "Epoch 332/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2749 - accuracy: 0.9092 - val_loss: 0.2800 - val_accuracy: 0.9165\n",
            "Epoch 333/500\n",
            "54/54 [==============================] - 2s 31ms/step - loss: 0.2776 - accuracy: 0.9081 - val_loss: 0.2824 - val_accuracy: 0.9173\n",
            "Epoch 334/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2734 - accuracy: 0.9065 - val_loss: 0.2827 - val_accuracy: 0.9187\n",
            "Epoch 335/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2775 - accuracy: 0.9068 - val_loss: 0.2842 - val_accuracy: 0.9179\n",
            "Epoch 336/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2699 - accuracy: 0.9100 - val_loss: 0.2756 - val_accuracy: 0.9194\n",
            "Epoch 337/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2713 - accuracy: 0.9099 - val_loss: 0.2794 - val_accuracy: 0.9173\n",
            "Epoch 338/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2719 - accuracy: 0.9094 - val_loss: 0.2772 - val_accuracy: 0.9179\n",
            "Epoch 339/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2645 - accuracy: 0.9106 - val_loss: 0.2728 - val_accuracy: 0.9183\n",
            "Epoch 340/500\n",
            "54/54 [==============================] - 2s 31ms/step - loss: 0.2657 - accuracy: 0.9108 - val_loss: 0.2781 - val_accuracy: 0.9182\n",
            "Epoch 341/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2665 - accuracy: 0.9093 - val_loss: 0.2792 - val_accuracy: 0.9159\n",
            "Epoch 342/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2650 - accuracy: 0.9121 - val_loss: 0.2766 - val_accuracy: 0.9190\n",
            "Epoch 343/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2632 - accuracy: 0.9129 - val_loss: 0.2776 - val_accuracy: 0.9180\n",
            "Epoch 344/500\n",
            "54/54 [==============================] - 2s 31ms/step - loss: 0.2664 - accuracy: 0.9115 - val_loss: 0.2736 - val_accuracy: 0.9186\n",
            "Epoch 345/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2627 - accuracy: 0.9118 - val_loss: 0.2713 - val_accuracy: 0.9201\n",
            "Epoch 346/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2584 - accuracy: 0.9142 - val_loss: 0.2727 - val_accuracy: 0.9187\n",
            "Epoch 347/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2586 - accuracy: 0.9135 - val_loss: 0.2697 - val_accuracy: 0.9200\n",
            "Epoch 348/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2547 - accuracy: 0.9144 - val_loss: 0.2681 - val_accuracy: 0.9197\n",
            "Epoch 349/500\n",
            "54/54 [==============================] - 2s 31ms/step - loss: 0.2514 - accuracy: 0.9163 - val_loss: 0.2678 - val_accuracy: 0.9215\n",
            "Epoch 350/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2559 - accuracy: 0.9145 - val_loss: 0.2676 - val_accuracy: 0.9187\n",
            "Epoch 351/500\n",
            "54/54 [==============================] - 2s 31ms/step - loss: 0.2524 - accuracy: 0.9178 - val_loss: 0.2710 - val_accuracy: 0.9212\n",
            "Epoch 352/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2553 - accuracy: 0.9169 - val_loss: 0.2637 - val_accuracy: 0.9211\n",
            "Epoch 353/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2508 - accuracy: 0.9167 - val_loss: 0.2663 - val_accuracy: 0.9202\n",
            "Epoch 354/500\n",
            "54/54 [==============================] - 2s 31ms/step - loss: 0.2502 - accuracy: 0.9168 - val_loss: 0.2680 - val_accuracy: 0.9187\n",
            "Epoch 355/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2478 - accuracy: 0.9165 - val_loss: 0.2640 - val_accuracy: 0.9232\n",
            "Epoch 356/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2442 - accuracy: 0.9188 - val_loss: 0.2666 - val_accuracy: 0.9201\n",
            "Epoch 357/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2420 - accuracy: 0.9176 - val_loss: 0.2624 - val_accuracy: 0.9209\n",
            "Epoch 358/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2452 - accuracy: 0.9198 - val_loss: 0.2644 - val_accuracy: 0.9200\n",
            "Epoch 359/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2458 - accuracy: 0.9161 - val_loss: 0.2601 - val_accuracy: 0.9230\n",
            "Epoch 360/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.2370 - accuracy: 0.9206 - val_loss: 0.2603 - val_accuracy: 0.9212\n",
            "Epoch 361/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2446 - accuracy: 0.9170 - val_loss: 0.2617 - val_accuracy: 0.9236\n",
            "Epoch 362/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2405 - accuracy: 0.9205 - val_loss: 0.2604 - val_accuracy: 0.9228\n",
            "Epoch 363/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2428 - accuracy: 0.9183 - val_loss: 0.2616 - val_accuracy: 0.9215\n",
            "Epoch 364/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2373 - accuracy: 0.9212 - val_loss: 0.2595 - val_accuracy: 0.9211\n",
            "Epoch 365/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2362 - accuracy: 0.9215 - val_loss: 0.2638 - val_accuracy: 0.9202\n",
            "Epoch 366/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2345 - accuracy: 0.9221 - val_loss: 0.2599 - val_accuracy: 0.9248\n",
            "Epoch 367/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2333 - accuracy: 0.9231 - val_loss: 0.2587 - val_accuracy: 0.9237\n",
            "Epoch 368/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2336 - accuracy: 0.9225 - val_loss: 0.2576 - val_accuracy: 0.9223\n",
            "Epoch 369/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2341 - accuracy: 0.9223 - val_loss: 0.2594 - val_accuracy: 0.9216\n",
            "Epoch 370/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2360 - accuracy: 0.9223 - val_loss: 0.2528 - val_accuracy: 0.9246\n",
            "Epoch 371/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2351 - accuracy: 0.9231 - val_loss: 0.2576 - val_accuracy: 0.9223\n",
            "Epoch 372/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2310 - accuracy: 0.9233 - val_loss: 0.2555 - val_accuracy: 0.9248\n",
            "Epoch 373/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2328 - accuracy: 0.9223 - val_loss: 0.2540 - val_accuracy: 0.9265\n",
            "Epoch 374/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2274 - accuracy: 0.9238 - val_loss: 0.2549 - val_accuracy: 0.9258\n",
            "Epoch 375/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2277 - accuracy: 0.9270 - val_loss: 0.2532 - val_accuracy: 0.9253\n",
            "Epoch 376/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2311 - accuracy: 0.9243 - val_loss: 0.2523 - val_accuracy: 0.9262\n",
            "Epoch 377/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2261 - accuracy: 0.9248 - val_loss: 0.2562 - val_accuracy: 0.9221\n",
            "Epoch 378/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2269 - accuracy: 0.9230 - val_loss: 0.2520 - val_accuracy: 0.9268\n",
            "Epoch 379/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2243 - accuracy: 0.9255 - val_loss: 0.2505 - val_accuracy: 0.9257\n",
            "Epoch 380/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2255 - accuracy: 0.9263 - val_loss: 0.2487 - val_accuracy: 0.9267\n",
            "Epoch 381/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2271 - accuracy: 0.9250 - val_loss: 0.2513 - val_accuracy: 0.9261\n",
            "Epoch 382/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2180 - accuracy: 0.9276 - val_loss: 0.2472 - val_accuracy: 0.9282\n",
            "Epoch 383/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2232 - accuracy: 0.9258 - val_loss: 0.2507 - val_accuracy: 0.9236\n",
            "Epoch 384/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2171 - accuracy: 0.9292 - val_loss: 0.2481 - val_accuracy: 0.9264\n",
            "Epoch 385/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2212 - accuracy: 0.9261 - val_loss: 0.2466 - val_accuracy: 0.9271\n",
            "Epoch 386/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2145 - accuracy: 0.9283 - val_loss: 0.2476 - val_accuracy: 0.9272\n",
            "Epoch 387/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2165 - accuracy: 0.9274 - val_loss: 0.2438 - val_accuracy: 0.9306\n",
            "Epoch 388/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2194 - accuracy: 0.9285 - val_loss: 0.2425 - val_accuracy: 0.9318\n",
            "Epoch 389/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2123 - accuracy: 0.9291 - val_loss: 0.2426 - val_accuracy: 0.9297\n",
            "Epoch 390/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2094 - accuracy: 0.9314 - val_loss: 0.2444 - val_accuracy: 0.9267\n",
            "Epoch 391/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2140 - accuracy: 0.9296 - val_loss: 0.2426 - val_accuracy: 0.9308\n",
            "Epoch 392/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2121 - accuracy: 0.9294 - val_loss: 0.2430 - val_accuracy: 0.9283\n",
            "Epoch 393/500\n",
            "54/54 [==============================] - 2s 28ms/step - loss: 0.2112 - accuracy: 0.9302 - val_loss: 0.2435 - val_accuracy: 0.9288\n",
            "Epoch 394/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2085 - accuracy: 0.9326 - val_loss: 0.2481 - val_accuracy: 0.9253\n",
            "Epoch 395/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2078 - accuracy: 0.9305 - val_loss: 0.2428 - val_accuracy: 0.9286\n",
            "Epoch 396/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2082 - accuracy: 0.9306 - val_loss: 0.2409 - val_accuracy: 0.9322\n",
            "Epoch 397/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2091 - accuracy: 0.9307 - val_loss: 0.2393 - val_accuracy: 0.9292\n",
            "Epoch 398/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2089 - accuracy: 0.9313 - val_loss: 0.2403 - val_accuracy: 0.9325\n",
            "Epoch 399/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2065 - accuracy: 0.9289 - val_loss: 0.2365 - val_accuracy: 0.9324\n",
            "Epoch 400/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.2071 - accuracy: 0.9323 - val_loss: 0.2359 - val_accuracy: 0.9332\n",
            "Epoch 401/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1996 - accuracy: 0.9344 - val_loss: 0.2379 - val_accuracy: 0.9300\n",
            "Epoch 402/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2047 - accuracy: 0.9335 - val_loss: 0.2374 - val_accuracy: 0.9303\n",
            "Epoch 403/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2080 - accuracy: 0.9311 - val_loss: 0.2399 - val_accuracy: 0.9297\n",
            "Epoch 404/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.2031 - accuracy: 0.9335 - val_loss: 0.2400 - val_accuracy: 0.9313\n",
            "Epoch 405/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1955 - accuracy: 0.9358 - val_loss: 0.2386 - val_accuracy: 0.9303\n",
            "Epoch 406/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1982 - accuracy: 0.9347 - val_loss: 0.2378 - val_accuracy: 0.9315\n",
            "Epoch 407/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1988 - accuracy: 0.9327 - val_loss: 0.2381 - val_accuracy: 0.9327\n",
            "Epoch 408/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1972 - accuracy: 0.9351 - val_loss: 0.2377 - val_accuracy: 0.9315\n",
            "Epoch 409/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1998 - accuracy: 0.9333 - val_loss: 0.2347 - val_accuracy: 0.9342\n",
            "Epoch 410/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1967 - accuracy: 0.9339 - val_loss: 0.2347 - val_accuracy: 0.9336\n",
            "Epoch 411/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1976 - accuracy: 0.9348 - val_loss: 0.2336 - val_accuracy: 0.9315\n",
            "Epoch 412/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1937 - accuracy: 0.9351 - val_loss: 0.2353 - val_accuracy: 0.9325\n",
            "Epoch 413/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1967 - accuracy: 0.9346 - val_loss: 0.2341 - val_accuracy: 0.9332\n",
            "Epoch 414/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1927 - accuracy: 0.9372 - val_loss: 0.2372 - val_accuracy: 0.9314\n",
            "Epoch 415/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1934 - accuracy: 0.9343 - val_loss: 0.2350 - val_accuracy: 0.9322\n",
            "Epoch 416/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1919 - accuracy: 0.9361 - val_loss: 0.2315 - val_accuracy: 0.9342\n",
            "Epoch 417/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1969 - accuracy: 0.9357 - val_loss: 0.2325 - val_accuracy: 0.9342\n",
            "Epoch 418/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1888 - accuracy: 0.9381 - val_loss: 0.2321 - val_accuracy: 0.9339\n",
            "Epoch 419/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1836 - accuracy: 0.9422 - val_loss: 0.2298 - val_accuracy: 0.9322\n",
            "Epoch 420/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1816 - accuracy: 0.9407 - val_loss: 0.2313 - val_accuracy: 0.9334\n",
            "Epoch 421/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1922 - accuracy: 0.9365 - val_loss: 0.2318 - val_accuracy: 0.9339\n",
            "Epoch 422/500\n",
            "54/54 [==============================] - 2s 31ms/step - loss: 0.1870 - accuracy: 0.9379 - val_loss: 0.2301 - val_accuracy: 0.9360\n",
            "Epoch 423/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1881 - accuracy: 0.9386 - val_loss: 0.2286 - val_accuracy: 0.9346\n",
            "Epoch 424/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1875 - accuracy: 0.9368 - val_loss: 0.2262 - val_accuracy: 0.9327\n",
            "Epoch 425/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1850 - accuracy: 0.9393 - val_loss: 0.2304 - val_accuracy: 0.9350\n",
            "Epoch 426/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1794 - accuracy: 0.9421 - val_loss: 0.2291 - val_accuracy: 0.9345\n",
            "Epoch 427/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1853 - accuracy: 0.9375 - val_loss: 0.2251 - val_accuracy: 0.9373\n",
            "Epoch 428/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1875 - accuracy: 0.9391 - val_loss: 0.2277 - val_accuracy: 0.9354\n",
            "Epoch 429/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1832 - accuracy: 0.9402 - val_loss: 0.2279 - val_accuracy: 0.9332\n",
            "Epoch 430/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1835 - accuracy: 0.9390 - val_loss: 0.2257 - val_accuracy: 0.9342\n",
            "Epoch 431/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1861 - accuracy: 0.9389 - val_loss: 0.2237 - val_accuracy: 0.9356\n",
            "Epoch 432/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1802 - accuracy: 0.9425 - val_loss: 0.2300 - val_accuracy: 0.9328\n",
            "Epoch 433/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1774 - accuracy: 0.9427 - val_loss: 0.2264 - val_accuracy: 0.9353\n",
            "Epoch 434/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1779 - accuracy: 0.9430 - val_loss: 0.2223 - val_accuracy: 0.9363\n",
            "Epoch 435/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1763 - accuracy: 0.9433 - val_loss: 0.2245 - val_accuracy: 0.9346\n",
            "Epoch 436/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1795 - accuracy: 0.9408 - val_loss: 0.2219 - val_accuracy: 0.9375\n",
            "Epoch 437/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1765 - accuracy: 0.9419 - val_loss: 0.2255 - val_accuracy: 0.9346\n",
            "Epoch 438/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1757 - accuracy: 0.9410 - val_loss: 0.2236 - val_accuracy: 0.9380\n",
            "Epoch 439/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1789 - accuracy: 0.9423 - val_loss: 0.2222 - val_accuracy: 0.9370\n",
            "Epoch 440/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1720 - accuracy: 0.9426 - val_loss: 0.2233 - val_accuracy: 0.9338\n",
            "Epoch 441/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1700 - accuracy: 0.9459 - val_loss: 0.2251 - val_accuracy: 0.9352\n",
            "Epoch 442/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1744 - accuracy: 0.9410 - val_loss: 0.2237 - val_accuracy: 0.9353\n",
            "Epoch 443/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1680 - accuracy: 0.9454 - val_loss: 0.2228 - val_accuracy: 0.9356\n",
            "Epoch 444/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1658 - accuracy: 0.9467 - val_loss: 0.2233 - val_accuracy: 0.9345\n",
            "Epoch 445/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1684 - accuracy: 0.9436 - val_loss: 0.2188 - val_accuracy: 0.9368\n",
            "Epoch 446/500\n",
            "54/54 [==============================] - 2s 30ms/step - loss: 0.1792 - accuracy: 0.9415 - val_loss: 0.2236 - val_accuracy: 0.9374\n",
            "Epoch 447/500\n",
            "54/54 [==============================] - 2s 29ms/step - loss: 0.1708 - accuracy: 0.9442 - val_loss: 0.2194 - val_accuracy: 0.9391\n",
            "Epoch 448/500\n",
            "53/54 [============================>.] - ETA: 0s - loss: 0.1666 - accuracy: 0.9467"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-1c821dbb8c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1223\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1226\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m   1321\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;34m\"\"\"Runs an evaluation execution with one step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m   1316\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1283\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1284\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1285\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2831\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2832\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2833\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2835\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3606\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3607\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3608\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3610\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `test_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1264\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m     \u001b[0;31m# Updates stateful loss metrics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     self.compiled_loss(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    378\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m  \u001b[0;31m# handle the corner case where self.layers is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \"\"\"\n\u001b[1;32m    420\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 421\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_save_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_functional_construction_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *exc_info)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m     \u001b[0mcall_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m     \u001b[0mcall_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_in_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0mcall_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "AGxukt1dh6hc",
        "outputId": "9611d8bc-4447-4210-8a8f-b9e895fd8025"
      },
      "source": [
        "plt.plot(model.history.history['loss'])\n",
        "plt.plot(model.history.history['val_loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7eff2e0eab90>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sfrtBWDjMJf"
      },
      "source": [
        "model.save('/content/drive/MyDrive/model_saves/model_lewis',save_format='h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
