{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYyu5uluK0Bb",
        "outputId": "525d433b-6e08-4fb4-f65d-238c1307aced"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import glob\n",
        "import re\n",
        "import math\n",
        "from time import time\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten\n",
        "# import tensorflow_hub as hub\n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx5mTKthsdbA"
      },
      "source": [
        "def calculate_mean_std(x, channels_first=False, verbose=0):\n",
        "    \"\"\"\n",
        "    Calculates channel-wise mean and std\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x : array\n",
        "        Array representing a collection of images (frames) or\n",
        "        collection of collections of images (frames) - namely video\n",
        "    channels_first : bool, optional\n",
        "        Leave False, by default False\n",
        "    verbose : int, optional\n",
        "        1-prints out details, 0-silent mode, by default 0\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    array of shape [2, num_channels]\n",
        "        Array with per channel mean and std for all the frames\n",
        "    \"\"\"\n",
        "    ndim = x.ndim\n",
        "    assert ndim in [5,4]\n",
        "    assert channels_first == False\n",
        "    all_mean = []\n",
        "    all_std = []    \n",
        "    num_channels = x.shape[-1]\n",
        "    \n",
        "    for c in range(0, num_channels):\n",
        "        if ndim ==5: # videos\n",
        "            mean = x[:,:,:,:,c].mean()\n",
        "            std = x[:,:,:,:,c].std()\n",
        "        elif ndim ==4: # images rgb or grayscale\n",
        "            mean = x[:,:,:,c].mean()\n",
        "            std = x[:,:,:,c].std()\n",
        "        if verbose:\n",
        "            print(\"Channel %s mean before: %s\" % (c, mean))   \n",
        "            print(\"Channel %s std before: %s\" % (c, std))\n",
        "            \n",
        "        all_mean.append(mean)\n",
        "        all_std.append(std)\n",
        "    \n",
        "    return np.stack((all_mean, all_std))\n",
        "\n",
        "\n",
        "def preprocess_input(x, mean_std, divide_std=False, channels_first=False, verbose=0):\n",
        "    \"\"\"\n",
        "    Channel-wise substraction of mean from the input and optional division by std\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x : array\n",
        "        Input array of images (frames) or videos\n",
        "    mean_std : array\n",
        "        Array of shape [2, num_channels] with per-channel mean and std\n",
        "    divide_std : bool, optional\n",
        "        Add division by std or not, by default False\n",
        "    channels_first : bool, optional\n",
        "        Leave False, otherwise not implemented, by default False\n",
        "    verbose : int, optional\n",
        "        1-prints out details, 0-silent mode, by default 0\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    array\n",
        "        Returns input array after applying preprocessing steps\n",
        "    \"\"\"\n",
        "    x = np.asarray(x, dtype=np.float32)    \n",
        "    ndim = x.ndim\n",
        "    assert ndim in [5,4]\n",
        "    assert channels_first == False\n",
        "    num_channels = x.shape[-1]\n",
        "    \n",
        "    for c in range(0, num_channels):  \n",
        "        if ndim ==5: # videos\n",
        "            x[:,:,:,:,c] -= mean_std[0][c]\n",
        "            if divide_std:\n",
        "                x[:,:,:,:,c] /= mean_std[1][c]\n",
        "            if verbose:\n",
        "                print(\"Channel %s mean after preprocessing: %s\" % (c, x[:,:,:,:,c].mean()))    \n",
        "                print(\"Channel %s std after preprocessing: %s\" % (c, x[:,:,:,:,c].std()))\n",
        "        elif ndim ==4: # images rgb or grayscale\n",
        "            x[:,:,:,c] -= mean_std[0][c]\n",
        "            if divide_std:\n",
        "                x[:,:,:,c] /= mean_std[1][c]   \n",
        "            if verbose:        \n",
        "                print(\"Channel %s mean after preprocessing: %s\" % (c, x[:,:,:,c].mean()))    \n",
        "                print(\"Channel %s std after preprocessing: %s\" % (c, x[:,:,:,c].std()))            \n",
        "    return x\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHPLaCsiseYc",
        "outputId": "58bddd1a-065e-464d-f048-f2193d7a948f"
      },
      "source": [
        "files = glob.glob('/content/drive/MyDrive/training_arr/*.avi') \n",
        "print(str(files[5]))\n",
        "\n",
        "training_labels = []\n",
        "training_files = []"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/training_arr/television10367_clipped.avi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awHX3a84sjA3"
      },
      "source": [
        "for file in files:\n",
        "    label = re.findall('[A-Za-z]+[0-9]',str(file))[0][:-1]\n",
        "    training_labels.append(label)\n",
        "    training_files.append(str(file))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNifm6DCsi8w",
        "outputId": "7e51512a-73f3-45fe-817d-e3d8846a5eb9"
      },
      "source": [
        "training_data = pd.DataFrame({'filename':training_files,'training_labels':training_labels})\n",
        "print(training_data)\n",
        "\n",
        "print(training_data)\n",
        "label_encoder = LabelEncoder().fit_transform(training_data['training_labels'])\n",
        "training_data['encoded_labels'] = label_encoder\n",
        "print(training_data)\n",
        "training_data.to_csv('training_words.csv')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               filename training_labels\n",
            "0     /content/drive/MyDrive/training_arr/teacher778...         teacher\n",
            "1     /content/drive/MyDrive/training_arr/teacher974...         teacher\n",
            "2     /content/drive/MyDrive/training_arr/teacher976...         teacher\n",
            "3     /content/drive/MyDrive/training_arr/teacher933...         teacher\n",
            "4     /content/drive/MyDrive/training_arr/teacher974...         teacher\n",
            "...                                                 ...             ...\n",
            "5328  /content/drive/MyDrive/training_arr/beautiful2...       beautiful\n",
            "5329  /content/drive/MyDrive/training_arr/beautiful2...       beautiful\n",
            "5330  /content/drive/MyDrive/training_arr/beautiful2...       beautiful\n",
            "5331  /content/drive/MyDrive/training_arr/beautiful8...       beautiful\n",
            "5332  /content/drive/MyDrive/training_arr/beautiful7...       beautiful\n",
            "\n",
            "[5333 rows x 2 columns]\n",
            "                                               filename training_labels\n",
            "0     /content/drive/MyDrive/training_arr/teacher778...         teacher\n",
            "1     /content/drive/MyDrive/training_arr/teacher974...         teacher\n",
            "2     /content/drive/MyDrive/training_arr/teacher976...         teacher\n",
            "3     /content/drive/MyDrive/training_arr/teacher933...         teacher\n",
            "4     /content/drive/MyDrive/training_arr/teacher974...         teacher\n",
            "...                                                 ...             ...\n",
            "5328  /content/drive/MyDrive/training_arr/beautiful2...       beautiful\n",
            "5329  /content/drive/MyDrive/training_arr/beautiful2...       beautiful\n",
            "5330  /content/drive/MyDrive/training_arr/beautiful2...       beautiful\n",
            "5331  /content/drive/MyDrive/training_arr/beautiful8...       beautiful\n",
            "5332  /content/drive/MyDrive/training_arr/beautiful7...       beautiful\n",
            "\n",
            "[5333 rows x 2 columns]\n",
            "                                               filename  ... encoded_labels\n",
            "0     /content/drive/MyDrive/training_arr/teacher778...  ...            248\n",
            "1     /content/drive/MyDrive/training_arr/teacher974...  ...            248\n",
            "2     /content/drive/MyDrive/training_arr/teacher976...  ...            248\n",
            "3     /content/drive/MyDrive/training_arr/teacher933...  ...            248\n",
            "4     /content/drive/MyDrive/training_arr/teacher974...  ...            248\n",
            "...                                                 ...  ...            ...\n",
            "5328  /content/drive/MyDrive/training_arr/beautiful2...  ...             24\n",
            "5329  /content/drive/MyDrive/training_arr/beautiful2...  ...             24\n",
            "5330  /content/drive/MyDrive/training_arr/beautiful2...  ...             24\n",
            "5331  /content/drive/MyDrive/training_arr/beautiful8...  ...             24\n",
            "5332  /content/drive/MyDrive/training_arr/beautiful7...  ...             24\n",
            "\n",
            "[5333 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnrheiXcsi5D"
      },
      "source": [
        "videos_data = []\n",
        "\n",
        "def gen_video_prep(file_loc, size):\n",
        "  \n",
        "\n",
        "    resize = size\n",
        "\n",
        "    cap = cv2.VideoCapture(str(file_loc))\n",
        "    ret = True\n",
        "      \n",
        "    frames=[]\n",
        "\n",
        "    while ret == True:\n",
        "        ret, frame = cap.read()\n",
        "        if ret == True:\n",
        "            frame = cv2.resize(frame,resize)\n",
        "            frame = frame / 255.0\n",
        "            frames.append(frame)\n",
        "          \n",
        "    video = np.stack(frames,axis=0)\n",
        "    frames, channels = video.shape[0], video.shape[3]\n",
        "\n",
        "    video = video[list(np.linspace(0,frames-1,32,dtype=int))]\n",
        "\n",
        "    #mean_std = calculate_mean_std(video, channels_first=False, verbose=0)\n",
        "\n",
        "    #video = preprocess_input(video, mean_std, divide_std=False, channels_first=False, verbose=0)\n",
        "\n",
        "    cap.release()\n",
        "      \n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "  \n",
        "    return video"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dta_-N-siyu"
      },
      "source": [
        "class My_Custom_Generator(tf.keras.utils.Sequence) :\n",
        "  \n",
        "  def __init__(self, video_filenames, labels, batch_size) :\n",
        "    self.video_filenames = video_filenames\n",
        "    self.labels = labels\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "    \n",
        "  def __len__(self) :\n",
        "    return (np.ceil(len(self.video_filenames) / float(self.batch_size))).astype(np.int)\n",
        "  \n",
        "  \n",
        "  def __getitem__(self, idx) :\n",
        "    batch_x = self.video_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
        "    \n",
        "    return np.array([\n",
        "            gen_video_prep(file_name, (224, 224)) for file_name in batch_x]), np.array(batch_y)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKmWpHSFsiql"
      },
      "source": [
        "y = np.asarray(training_data['encoded_labels'].values)\n",
        "y = to_categorical(y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G_WYw_3sig-"
      },
      "source": [
        "gen = My_Custom_Generator(training_data['filename'].tolist(), y, 32)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyWjFUtSLnUh"
      },
      "source": [
        "img_feature_layer = hub.KerasLayer('https://tfhub.dev/deepmind/i3d-kinetics-600/1', input_shape = (32,224,224,3), trainable = False) #, input_shape=(40,224,224,3)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFiRz3frRSon"
      },
      "source": [
        "model = tf.keras.Sequential([img_feature_layer, \n",
        "                             tf.keras.layers.Dense(512, activation=\"relu\"), \n",
        "                             tf.keras.layers.Dropout(0.3), \n",
        "                             tf.keras.layers.Dense(298, activation=\"softmax\")])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW55oY6UWjry"
      },
      "source": [
        "model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WUOJYSXY7TY",
        "outputId": "284fdbce-218f-4a8a-dbc6-1811263b07a1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 600)               12909544  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               307712    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 298)               152874    \n",
            "=================================================================\n",
            "Total params: 13,370,130\n",
            "Trainable params: 460,586\n",
            "Non-trainable params: 12,909,544\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERIq9hhAY9mI",
        "outputId": "9a8599e3-77de-4008-b91f-ed24e73cf9d4"
      },
      "source": [
        "model.fit(\n",
        "        gen,\n",
        "        epochs=200,\n",
        "        batch_size=32\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "167/167 [==============================] - 1987s 12s/step - loss: 6.4639 - accuracy: 5.6254e-04\n",
            "Epoch 2/200\n",
            "167/167 [==============================] - 568s 3s/step - loss: 5.6688 - accuracy: 0.0081\n",
            "Epoch 3/200\n",
            "167/167 [==============================] - 570s 3s/step - loss: 5.6276 - accuracy: 0.0111\n",
            "Epoch 4/200\n",
            "167/167 [==============================] - 572s 3s/step - loss: 5.5647 - accuracy: 0.0206\n",
            "Epoch 5/200\n",
            "167/167 [==============================] - 574s 3s/step - loss: 5.5081 - accuracy: 0.0253\n",
            "Epoch 6/200\n",
            "167/167 [==============================] - 573s 3s/step - loss: 5.4022 - accuracy: 0.0407\n",
            "Epoch 7/200\n",
            "167/167 [==============================] - 573s 3s/step - loss: 5.3394 - accuracy: 0.0446\n",
            "Epoch 8/200\n",
            "167/167 [==============================] - 574s 3s/step - loss: 5.2860 - accuracy: 0.0456\n",
            "Epoch 9/200\n",
            "167/167 [==============================] - 575s 3s/step - loss: 5.1456 - accuracy: 0.0619\n",
            "Epoch 10/200\n",
            "167/167 [==============================] - 575s 3s/step - loss: 5.0620 - accuracy: 0.0591\n",
            "Epoch 11/200\n",
            "167/167 [==============================] - 575s 3s/step - loss: 4.9738 - accuracy: 0.0626\n",
            "Epoch 12/200\n",
            "167/167 [==============================] - 576s 3s/step - loss: 4.8732 - accuracy: 0.0763\n",
            "Epoch 13/200\n",
            "167/167 [==============================] - 576s 3s/step - loss: 4.7824 - accuracy: 0.0885\n",
            "Epoch 14/200\n",
            "167/167 [==============================] - 577s 3s/step - loss: 4.6798 - accuracy: 0.0953\n",
            "Epoch 15/200\n",
            "167/167 [==============================] - 578s 3s/step - loss: 4.5939 - accuracy: 0.1037\n",
            "Epoch 16/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 4.5059 - accuracy: 0.1054\n",
            "Epoch 17/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 4.4217 - accuracy: 0.1198\n",
            "Epoch 18/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 4.3267 - accuracy: 0.1333\n",
            "Epoch 19/200\n",
            "167/167 [==============================] - 578s 3s/step - loss: 4.2129 - accuracy: 0.1401\n",
            "Epoch 20/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 4.1480 - accuracy: 0.1547\n",
            "Epoch 21/200\n",
            "167/167 [==============================] - 580s 3s/step - loss: 4.0731 - accuracy: 0.1491\n",
            "Epoch 22/200\n",
            "167/167 [==============================] - 580s 3s/step - loss: 3.9991 - accuracy: 0.1648\n",
            "Epoch 23/200\n",
            "167/167 [==============================] - 581s 3s/step - loss: 3.9089 - accuracy: 0.1779\n",
            "Epoch 24/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 3.8191 - accuracy: 0.1838\n",
            "Epoch 25/200\n",
            "167/167 [==============================] - 578s 3s/step - loss: 3.7251 - accuracy: 0.1926\n",
            "Epoch 26/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 3.6852 - accuracy: 0.2010\n",
            "Epoch 27/200\n",
            "167/167 [==============================] - 581s 3s/step - loss: 3.5998 - accuracy: 0.2136\n",
            "Epoch 28/200\n",
            "167/167 [==============================] - 581s 3s/step - loss: 3.5715 - accuracy: 0.2132\n",
            "Epoch 29/200\n",
            "167/167 [==============================] - 582s 3s/step - loss: 3.4562 - accuracy: 0.2303\n",
            "Epoch 30/200\n",
            "167/167 [==============================] - 581s 3s/step - loss: 3.4098 - accuracy: 0.2453\n",
            "Epoch 31/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 3.3628 - accuracy: 0.2451\n",
            "Epoch 32/200\n",
            "167/167 [==============================] - 578s 3s/step - loss: 3.3251 - accuracy: 0.2490\n",
            "Epoch 33/200\n",
            "167/167 [==============================] - 578s 3s/step - loss: 3.2196 - accuracy: 0.2674\n",
            "Epoch 34/200\n",
            "167/167 [==============================] - 577s 3s/step - loss: 3.2477 - accuracy: 0.2569\n",
            "Epoch 35/200\n",
            "167/167 [==============================] - 577s 3s/step - loss: 3.1447 - accuracy: 0.2785\n",
            "Epoch 36/200\n",
            "167/167 [==============================] - 577s 3s/step - loss: 3.0988 - accuracy: 0.2794\n",
            "Epoch 37/200\n",
            "167/167 [==============================] - 577s 3s/step - loss: 3.0199 - accuracy: 0.2931\n",
            "Epoch 38/200\n",
            "167/167 [==============================] - 577s 3s/step - loss: 3.0070 - accuracy: 0.3015\n",
            "Epoch 39/200\n",
            "167/167 [==============================] - 576s 3s/step - loss: 2.9983 - accuracy: 0.2955\n",
            "Epoch 40/200\n",
            "167/167 [==============================] - 577s 3s/step - loss: 2.9606 - accuracy: 0.2981\n",
            "Epoch 41/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 2.8589 - accuracy: 0.3178\n",
            "Epoch 42/200\n",
            "167/167 [==============================] - 580s 3s/step - loss: 2.8395 - accuracy: 0.3205\n",
            "Epoch 43/200\n",
            "167/167 [==============================] - 583s 3s/step - loss: 2.8194 - accuracy: 0.3220\n",
            "Epoch 44/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 2.7792 - accuracy: 0.3328\n",
            "Epoch 45/200\n",
            "167/167 [==============================] - 581s 3s/step - loss: 2.7011 - accuracy: 0.3531\n",
            "Epoch 46/200\n",
            "167/167 [==============================] - 580s 3s/step - loss: 2.6685 - accuracy: 0.3542\n",
            "Epoch 47/200\n",
            "167/167 [==============================] - 581s 3s/step - loss: 2.6335 - accuracy: 0.3585\n",
            "Epoch 48/200\n",
            "167/167 [==============================] - 582s 3s/step - loss: 2.6181 - accuracy: 0.3581\n",
            "Epoch 49/200\n",
            "167/167 [==============================] - 586s 4s/step - loss: 2.5844 - accuracy: 0.3641\n",
            "Epoch 50/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 2.5410 - accuracy: 0.3756\n",
            "Epoch 51/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 2.5366 - accuracy: 0.3775\n",
            "Epoch 52/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 2.5229 - accuracy: 0.3776\n",
            "Epoch 53/200\n",
            "167/167 [==============================] - 579s 3s/step - loss: 2.4688 - accuracy: 0.3765\n",
            "Epoch 54/200\n",
            "167/167 [==============================] - 580s 3s/step - loss: 2.4483 - accuracy: 0.3891\n",
            "Epoch 55/200\n",
            "167/167 [==============================] - 583s 3s/step - loss: 2.3698 - accuracy: 0.4095\n",
            "Epoch 56/200\n",
            "167/167 [==============================] - 582s 3s/step - loss: 2.3469 - accuracy: 0.4082\n",
            "Epoch 57/200\n",
            "167/167 [==============================] - 582s 3s/step - loss: 2.3314 - accuracy: 0.4148\n",
            "Epoch 58/200\n",
            "167/167 [==============================] - 582s 3s/step - loss: 2.3487 - accuracy: 0.4120\n",
            "Epoch 59/200\n",
            "167/167 [==============================] - 582s 3s/step - loss: 2.2578 - accuracy: 0.4243\n",
            "Epoch 60/200\n",
            "167/167 [==============================] - 583s 3s/step - loss: 2.2906 - accuracy: 0.4200\n",
            "Epoch 61/200\n",
            " 97/167 [================>.............] - ETA: 4:03 - loss: 2.1306 - accuracy: 0.4572"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4JtqMGauBVW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}